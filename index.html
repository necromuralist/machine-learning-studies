<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article# " lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="Notes on Machine Learning">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Machine Learning Studies</title>
<link href="assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" href="rss.xml">
<link rel="canonical" href="http://necromuralist.github.io/machine-learning-studies/">
<!--[if lt IE 9]><script src="assets/js/html5.js"></script><![endif]--><script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script><link rel="prefetch" href="posts/data-sources/" type="text/html">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-default navbar-static-top"><div class="container">
<!-- This keeps the margins nice -->
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="http://necromuralist.github.io/machine-learning-studies/">

                <span id="blog-title">Machine Learning Studies</span>
            </a>
        </div>
<!-- /.navbar-header -->
        <div class="collapse navbar-collapse" id="bs-navbar" aria-expanded="false">
            <ul class="nav navbar-nav">
<li>
<a href="https://necromuralist.github.io/">The Cloistered Monkey</a>
                </li>
<li>
<a href="archive.html">Archive</a>
                </li>
<li>
<a href="categories/">Tags</a>
                </li>
<li>
<a href="rss.xml">RSS feed</a>

                
            </li>
</ul>
<!-- Google custom search --><form method="get" action="https://www.google.com/search" class="navbar-form navbar-right" role="search">
<div class="form-group">
<input type="text" name="q" class="form-control" placeholder="Search">
</div>
<button type="submit" class="btn btn-primary">
	<span class="glyphicon glyphicon-search"></span>
</button>
<input type="hidden" name="sitesearch" value="http://necromuralist.github.io/machine-learning-studies/">
</form>
<!-- End of custom search -->


            <ul class="nav navbar-nav navbar-right"></ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        <div class="row">
            
            

    


    
<div class="postindex">
    <article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/data-sources/" class="u-url">Data Sources</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="authors/necromuralist/">necromuralist</a>
            </span></p>
            <p class="dateline"><a href="posts/data-sources/" rel="bookmark"><time class="published dt-published" datetime="2018-08-01T12:25:56-07:00" title="2018-08-01 12:25">2018-08-01 12:25</time></a></p>
        </div>
    </header><div class="e-content entry-content">
    <div id="outline-container-org39e72ee" class="outline-2">
<h2 id="org39e72ee">Open Repositories</h2>
<div class="outline-text-2" id="text-org39e72ee">
<ul class="org-ul">
<li>
<a href="http://scikit-learn.org/stable/datasets/index.html#datasets">Sklearn</a> : not a repository, per se, but these are the datasets that you can get from sklearn.datasets (not all of them are mentioned, though (e.g. <code>fetch_califoria_housing</code>))</li>
<li><a href="https://archive.ics.uci.edu/ml/index.php">UC Irvine Machine Learning Repository</a></li>
<li><a href="https://www.kaggle.com/datasets">Kaggle Datasets</a></li>
<li><a href="https://registry.opendata.aws/">Registry of Open Data on AWS</a></li>
</ul>
</div>
</div>
<div id="outline-container-org0934d0f" class="outline-2">
<h2 id="org0934d0f">Data Portals</h2>
<div class="outline-text-2" id="text-org0934d0f">
<ul class="org-ul">
<li>
<a href="http://dataportals.org/">Data Portals</a> : A Comprehensive List of Open Data Portals Around the World</li>
<li>
<a href="https://opendatamonitor.eu/frontend/web/index.php?r=dashboard/index">Open Data Monitor</a> : Europead Open Data</li>
<li>
<a href="https://www.quandl.com/">Quandl</a>: Data for investors</li>
</ul>
</div>
</div>
<div id="outline-container-org439ff47" class="outline-2">
<h2 id="org439ff47">Pages Listing Data Sets</h2>
<div class="outline-text-2" id="text-org439ff47">
<ul class="org-ul">
<li>
<a href="https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research">Wikipedia</a>: List of datasets for machine learning research</li>
<li>
<a href="https://www.quora.com/Where-can-I-find-large-datasets-open-to-the-public">Quora</a>: Where can I find large datasets open to the public?</li>
<li>
<a href="https://www.reddit.com/r/datasets/">Reddit</a>: r/datasets</li>
</ul>
</div>
</div>
    </div>
    </article><article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/references/" class="u-url">References</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="authors/necromuralist/">necromuralist</a>
            </span></p>
            <p class="dateline"><a href="posts/references/" rel="bookmark"><time class="published dt-published" datetime="2018-07-30T16:57:06-07:00" title="2018-07-30 16:57">2018-07-30 16:57</time></a></p>
        </div>
    </header><div class="e-content entry-content">
    <div id="outline-container-orga9826fe" class="outline-2">
<h2 id="orga9826fe">Short Codes</h2>
<div class="outline-text-2" id="text-orga9826fe">
<p>
These are short-hand codes to make it easier to quickly refer to sources.
</p>

<ul class="org-ul">
<li>[HOML]: Hands-On Machine Learning with Scikit-Learn and TensorFlow</li>
</ul>
</div>
</div>
<div id="outline-container-orgfbc1035" class="outline-2">
<h2 id="orgfbc1035">Bibliography</h2>
<div class="outline-text-2" id="text-orgfbc1035">
<ul class="org-ul">
<li>Géron, Aurélien. Hands-on Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems. First edition. Beijing Boston Farnham: O’Reilly, 2017.</li>
</ul>
</div>
</div>
    </div>
    </article><article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/california-housing-prices/" class="u-url">California Housing Prices</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="authors/necromuralist/">necromuralist</a>
            </span></p>
            <p class="dateline"><a href="posts/california-housing-prices/" rel="bookmark"><time class="published dt-published" datetime="2018-07-30T16:54:39-07:00" title="2018-07-30 16:54">2018-07-30 16:54</time></a></p>
        </div>
    </header><div class="e-content entry-content">
    <div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/california-housing-prices/#orgea62352">Introduction</a></li>
<li><a href="posts/california-housing-prices/#org1ed1d50">Imports</a></li>
<li><a href="posts/california-housing-prices/#orgaaeef10">Constants</a></li>
<li><a href="posts/california-housing-prices/#org93ea195">The Data</a></li>
<li><a href="posts/california-housing-prices/#orgd19ec42">References</a></li>
</ul>
</div>
</div>
<div id="outline-container-orgea62352" class="outline-2">
<h2 id="orgea62352">Introduction</h2>
<div class="outline-text-2" id="text-orgea62352">
<p>
This is an introductory regression problem that uses California housing data from the 1990 census. There's a description of the original data <a href="https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.htm">here</a>, but we're using a slightly altered dataset that's <a href="https://github.com/ageron/handson-ml/tree/master/datasets/housing">on github</a> (and appears to be mirrored <a href="https://www.kaggle.com/camnugent/california-housing-prices">on kaggle</a>). The problem here is to create a model that will predict the median housing value for a census block group (called "district" in the dataset) given the other attributes. The original data is also available from <a href="http://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html">sklearn</a> so I'm going to take advantage of that to get the description and do a double-check of the model.
</p>
</div>
</div>

<div id="outline-container-org1ed1d50" class="outline-2">
<h2 id="org1ed1d50">Imports</h2>
<div class="outline-text-2" id="text-org1ed1d50">
<p>
These are the dependencies for this problem.
</p>

<div class="highlight"><pre><span></span># python standard library
import os
import tarfile
import warnings
warnings.filterwarnings("ignore", message="numpy.dtype size changed")
from http import HTTPStatus

# from pypi
import matplotlib
import pandas
import requests
import seaborn
from sklearn.datasets import fetch_california_housing
from tabulate import tabulate
</pre></div>
</div>
</div>

<div id="outline-container-orgaaeef10" class="outline-2">
<h2 id="orgaaeef10">Constants</h2>
<div class="outline-text-2" id="text-orgaaeef10">
<p>
These are convenience holders for strings and other constants so they don't get scattered all over the place.
</p>

<div class="highlight"><pre><span></span>class Data:
    source_slug = "../data/california-housing-prices/"
    target_slug = "../data_temp/california-housing-prices/"
    url = "https://github.com/ageron/handson-ml/raw/master/datasets/housing/housing.tgz"
    source = source_slug + "housing.tgz"
    target = target_slug + "housing.csv"
    chunk_size = 128
</pre></div>
</div>
</div>
<div id="outline-container-org93ea195" class="outline-2">
<h2 id="org93ea195">The Data</h2>
<div class="outline-text-2" id="text-org93ea195">
<p>
We'll grab the data from github, extract it (it's a <code>tgz</code> compressed tarfile), then make a pandas data frame from it. I'll also download the <code>sklearn</code> version.
</p>
</div>
<div id="outline-container-org0b2d704" class="outline-3">
<h3 id="org0b2d704">Downloading the sklearn dataset</h3>
<div class="outline-text-3" id="text-org0b2d704">
<div class="highlight"><pre><span></span>sklearn_housing_bunch = fetch_california_housing("~/data/sklearn_datasets/")
</pre></div>

<pre class="example">
Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to /home/brunhilde/data/sklearn_datasets/
</pre>

<div class="highlight"><pre><span></span>print(sklearn_housing_bunch.DESCR)
</pre></div>

<pre class="example">
California housing dataset.

The original database is available from StatLib

    http://lib.stat.cmu.edu/datasets/

The data contains 20,640 observations on 9 variables.

This dataset contains the average house value as target variable
and the following input variables (features): average income,
housing average age, average rooms, average bedrooms, population,
average occupation, latitude, and longitude in that order.

References
----------

Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,
Statistics and Probability Letters, 33 (1997) 291-297.


</pre>

<div class="highlight"><pre><span></span>print(sklearn_housing_bunch.feature_names)
</pre></div>

<p>
['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']
</p>

<p>
Now I'll convert it to a Pandas DataFrame.
</p>


<div class="highlight"><pre><span></span>sklearn_housing = pandas.DataFrame(sklearn_housing_bunch.data,
				   columns=sklearn_housing_bunch.feature_names)
</pre></div>

<pre class="example">
             MedInc      HouseAge      AveRooms     AveBedrms    Population  \
count  20640.000000  20640.000000  20640.000000  20640.000000  20640.000000   
mean       3.870671     28.639486      5.429000      1.096675   1425.476744   
std        1.899822     12.585558      2.474173      0.473911   1132.462122   
min        0.499900      1.000000      0.846154      0.333333      3.000000   
25%        2.563400     18.000000      4.440716      1.006079    787.000000   
50%        3.534800     29.000000      5.229129      1.048780   1166.000000   
75%        4.743250     37.000000      6.052381      1.099526   1725.000000   
max       15.000100     52.000000    141.909091     34.066667  35682.000000   

           AveOccup      Latitude     Longitude  
count  20640.000000  20640.000000  20640.000000  
mean       3.070655     35.631861   -119.569704  
std       10.386050      2.135952      2.003532  
min        0.692308     32.540000   -124.350000  
25%        2.429741     33.930000   -121.800000  
50%        2.818116     34.260000   -118.490000  
75%        3.282261     37.710000   -118.010000  
max     1243.333333     41.950000   -114.310000  
</pre>
</div>
</div>

<div id="outline-container-org44d2659" class="outline-3">
<h3 id="org44d2659">Downloading and uncompressing the data</h3>
<div class="outline-text-3" id="text-org44d2659">
<div class="highlight"><pre><span></span>def get_data():
    """Gets the data from github and uncompresses it"""
    if os.path.exists(Data.target):
	return

    os.makedirs(Data.target_slug, exist_ok=True)
    os.makedirs(Data.source_slug, exist_ok=True)
    response = requests.get(Data.url, stream=True)
    assert response.status_code == HTTPStatus.OK
    with open(Data.source, "wb") as writer:
	for chunk in response.iter_content(chunk_size=Data.chunk_size):
	    writer.write(chunk)
    assert os.path.exists(Data.source)
    compressed = tarfile.open(Data.source)
    compressed.extractall(Data.target_slug)
    compressed.close()
    assert os.path.exists(Data.target)
    return
</pre></div>


<p>
Contents of ../data_temp/california-housing-prices/:
</p>
<ul class="org-ul">
<li>housing.csv</li>
</ul>
</div>
</div>

<div id="outline-container-orge6cc6a9" class="outline-3">
<h3 id="orge6cc6a9">Building the dataframe</h3>
<div class="outline-text-3" id="text-orge6cc6a9">
<div class="highlight"><pre><span></span>housing = pandas.read_csv(Data.target)
</pre></div>

<pre class="example">
&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 20640 entries, 0 to 20639
Data columns (total 10 columns):
longitude             20640 non-null float64
latitude              20640 non-null float64
housing_median_age    20640 non-null float64
total_rooms           20640 non-null float64
total_bedrooms        20433 non-null float64
population            20640 non-null float64
households            20640 non-null float64
median_income         20640 non-null float64
median_house_value    20640 non-null float64
ocean_proximity       20640 non-null object
dtypes: float64(9), object(1)
memory usage: 1.6+ MB
None
</pre>
</div>
</div>

<div id="outline-container-org5049e7c" class="outline-3">
<h3 id="org5049e7c">Comparison to Sklearn</h3>
<div class="outline-text-3" id="text-org5049e7c">
<p>
The dataset seems to differ somewhat from the sklearn description. Instead of <code>total_rooms</code> they have <code>AveRooms</code>, for instance. Is this just a problem of names?
</p>

<div class="highlight"><pre><span></span>print(sklearn_housing.AveRooms.head())
</pre></div>

<p>
0    6.984127
1    6.238137
2    8.288136
3    5.817352
4    6.281853
Name: AveRooms, dtype: float64
</p>

<div class="highlight"><pre><span></span>print(housing.total_rooms.head())
</pre></div>

<p>
0     880.0
1    7099.0
2    1467.0
3    1274.0
4    1627.0
Name: total_rooms, dtype: float64
</p>

<p>
So they are different. Let's see if you can get the sklearn values from the original data set.
</p>

<div class="highlight"><pre><span></span>print((housing.total_rooms/housing.households).head())
</pre></div>

<p>
0    6.984127
1    6.238137
2    8.288136
3    5.817352
4    6.281853
dtype: float64
</p>

<p>
It looks like the sklearn values are (in some cases) calculated values derived from the original. It makes sense that they changed some of the things (total number of rooms only makes sense if there is the same number of households in each district, for instance), but it would have been better if they documented the changes they made and why they changed it.
</p>
</div>
</div>

<div id="outline-container-orga5496df" class="outline-3">
<h3 id="orga5496df">Inspecting the Data</h3>
<div class="outline-text-3" id="text-orga5496df">
<p>
If you look at the <code>total_bedrooms</code> count you'll see that it only has 20,433 non-null values, while the rest of the columns have 20,640 values. These were removed to allow experimenting with missing data. The original dataset that was collected for the census had all the values.
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<colgroup>
<col class="org-left">
<col class="org-left">
</colgroup>
<thead><tr>
<th scope="col" class="org-left">Column</th>
<th scope="col" class="org-left">Has Missing Values</th>
</tr></thead>
<tbody>
<tr>
<td class="org-left">longitude</td>
<td class="org-left">False</td>
</tr>
<tr>
<td class="org-left">latitude</td>
<td class="org-left">False</td>
</tr>
<tr>
<td class="org-left">housing_median_age</td>
<td class="org-left">False</td>
</tr>
<tr>
<td class="org-left">total_rooms</td>
<td class="org-left">False</td>
</tr>
<tr>
<td class="org-left">total_bedrooms</td>
<td class="org-left">True</td>
</tr>
<tr>
<td class="org-left">population</td>
<td class="org-left">False</td>
</tr>
<tr>
<td class="org-left">households</td>
<td class="org-left">False</td>
</tr>
<tr>
<td class="org-left">median_income</td>
<td class="org-left">False</td>
</tr>
<tr>
<td class="org-left">median_house_value</td>
<td class="org-left">False</td>
</tr>
<tr>
<td class="org-left">ocean_proximity</td>
<td class="org-left">False</td>
</tr>
</tbody>
</table>
<p>
It looks like <code>total_bedrooms</code> is the only column where there's missing data.
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<colgroup>
<col class="org-right">
<col class="org-right">
</colgroup>
<thead><tr>
<th scope="col" class="org-right">Rows</th>
<th scope="col" class="org-right">Columns</th>
</tr></thead>
<tbody><tr>
<td class="org-right">20640</td>
<td class="org-right">10</td>
</tr></tbody>
</table>
<p>
I'll print the median for each column except the last (since it's non-numeric).
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<colgroup>
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
</colgroup>
<thead><tr>
<th scope="col" class="org-right">longitude</th>
<th scope="col" class="org-right">latitude</th>
<th scope="col" class="org-right">housing_median_age</th>
<th scope="col" class="org-right">total_rooms</th>
</tr></thead>
<tbody><tr>
<td class="org-right">-118.49</td>
<td class="org-right">34.26</td>
<td class="org-right">29.00</td>
<td class="org-right">2127.00</td>
</tr></tbody>
</table>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<colgroup>
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
</colgroup>
<thead><tr>
<th scope="col" class="org-right">total_bedrooms</th>
<th scope="col" class="org-right">population</th>
<th scope="col" class="org-right">households</th>
<th scope="col" class="org-right">median_income</th>
<th scope="col" class="org-right">median_house_value</th>
</tr></thead>
<tbody><tr>
<td class="org-right">435.00</td>
<td class="org-right">1166.00</td>
<td class="org-right">409.00</td>
<td class="org-right">3.53</td>
<td class="org-right">179700.00</td>
</tr></tbody>
</table>
<p>
Here's the description for the <code>ocean_proximity</code> variable
Looking at the <code>median_income</code> you can see that it isn't income in dollars.
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<colgroup>
<col class="org-left">
<col class="org-right">
</colgroup>
<thead><tr>
<th scope="col" class="org-left">Statistic</th>
<th scope="col" class="org-right">Value</th>
</tr></thead>
<tbody>
<tr>
<td class="org-left">count</td>
<td class="org-right">20640</td>
</tr>
<tr>
<td class="org-left">unique</td>
<td class="org-right">5</td>
</tr>
<tr>
<td class="org-left">top</td>
<td class="org-right">&lt;1H OCEAN</td>
</tr>
<tr>
<td class="org-left">freq</td>
<td class="org-right">9136</td>
</tr>
</tbody>
</table>
<p>
It looks like the most common house location is less than an hour from the ocean.
</p>

<div class="highlight"><pre><span></span>print(
    "{:.2f}".format(
	ocean_proximity_description.loc["freq"]/ocean_proximity_description.loc["count"]))
</pre></div>

<p>
0.44
</p>

<p>
Which makes up about forty-four percent of all the houses. Here are all the <code>ocean_proximity</code> values.
</p>


<div class="figure">
<p><img src="posts/california-housing-prices/ocean_proximity.png" alt="ocean_proximity.png"></p>
</div>


<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<colgroup>
<col class="org-left">
<col class="org-right">
<col class="org-right">
</colgroup>
<thead><tr>
<th scope="col" class="org-left">Proximity</th>
<th scope="col" class="org-right">Count</th>
<th scope="col" class="org-right">Percentage</th>
</tr></thead>
<tbody>
<tr>
<td class="org-left">&lt;1H OCEAN</td>
<td class="org-right">9136</td>
<td class="org-right">44.2636</td>
</tr>
<tr>
<td class="org-left">INLAND</td>
<td class="org-right">6551</td>
<td class="org-right">31.7393</td>
</tr>
<tr>
<td class="org-left">NEAR OCEAN</td>
<td class="org-right">2658</td>
<td class="org-right">12.8779</td>
</tr>
<tr>
<td class="org-left">NEAR BAY</td>
<td class="org-right">2290</td>
<td class="org-right">11.095</td>
</tr>
<tr>
<td class="org-left">ISLAND</td>
<td class="org-right">5</td>
<td class="org-right">0.0242248</td>
</tr>
</tbody>
</table>
<div class="figure">
<p><img src="posts/california-housing-prices/housing_histogram.png" alt="housing_histogram.png"></p>
</div>


<p>
If you look at the median income plot you can see that it goes from 0 to 15. It turns out that the incomes were re-scaled and limited to the 0.5 to 15 range. The median age and value were also capped, possibly affecting our price predictions.
</p>
</div>
</div>
</div>
<div id="outline-container-orgd19ec42" class="outline-2">
<h2 id="orgd19ec42">References</h2>
<div class="outline-text-2" id="text-orgd19ec42">
<ul class="org-ul">
<li>Géron, Aurélien. Hands-on Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems. First edition. Beijing Boston Farnham: O’Reilly, 2017.</li>
</ul>
</div>
</div>
    </div>
    </article><article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/decision-tree-classification/" class="u-url">Decision Tree Classification</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="authors/brunhilde/">Brunhilde</a>
            </span></p>
            <p class="dateline"><a href="posts/decision-tree-classification/" rel="bookmark"><time class="published dt-published" datetime="2017-07-17T16:41:00-07:00" title="2017-07-17 16:41">2017-07-17 16:41</time></a></p>
        </div>
    </header><div class="e-content entry-content">
    <div>
<div class="section" id="imports">
<h2>1 Imports</h2>
<pre class="code python"><a name="rest_code_764203a372d54898b6b964a3ad06667d-1"></a><span class="kn">import</span> <span class="nn">graphviz</span>
<a name="rest_code_764203a372d54898b6b964a3ad06667d-2"></a><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="p">(</span>
<a name="rest_code_764203a372d54898b6b964a3ad06667d-3"></a>    <span class="n">DecisionTreeClassifier</span><span class="p">,</span>
<a name="rest_code_764203a372d54898b6b964a3ad06667d-4"></a>    <span class="n">export_graphviz</span><span class="p">,</span>
<a name="rest_code_764203a372d54898b6b964a3ad06667d-5"></a>    <span class="p">)</span>
<a name="rest_code_764203a372d54898b6b964a3ad06667d-6"></a><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<a name="rest_code_764203a372d54898b6b964a3ad06667d-7"></a><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_breast_cancer</span>
</pre>
<pre class="code python"><a name="rest_code_6f5c259b69d54e7a8d63f147524135dc-1"></a><span class="o">%</span> <span class="n">matplotlib</span> <span class="n">inline</span>
</pre>
</div>
<div class="section" id="the-data">
<h2>2 The Data</h2>
<pre class="code python"><a name="rest_code_5832a518a5874376b739bfa9634c9824-1"></a><span class="n">cancer</span> <span class="o">=</span> <span class="n">load_breast_cancer</span><span class="p">()</span>
<a name="rest_code_5832a518a5874376b739bfa9634c9824-2"></a><span class="k">print</span><span class="p">(</span><span class="n">cancer</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
</pre>
<pre class="literal-block">
dict_keys(['DESCR', 'target_names', 'data', 'feature_names', 'target'])
</pre>
<pre class="code python"><a name="rest_code_cbaa488eb6af40fdb66c1fd05832a96a-1"></a><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">cancer</span><span class="o">.</span><span class="n">data</span><span class="p">,</span>
<a name="rest_code_cbaa488eb6af40fdb66c1fd05832a96a-2"></a>                                                    <span class="n">cancer</span><span class="o">.</span><span class="n">target</span><span class="p">,</span>
<a name="rest_code_cbaa488eb6af40fdb66c1fd05832a96a-3"></a>                                                    <span class="n">stratify</span><span class="o">=</span><span class="n">cancer</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
</pre>
</div>
<div class="section" id="the-model">
<h2>3 The Model</h2>
<pre class="code python"><a name="rest_code_0b3882ea21484784956111318de886c7-1"></a><span class="k">def</span> <span class="nf">build_tree</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
<a name="rest_code_0b3882ea21484784956111318de886c7-2"></a>    <span class="n">tree</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="n">max_depth</span><span class="p">)</span>
<a name="rest_code_0b3882ea21484784956111318de886c7-3"></a>    <span class="n">tree</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<a name="rest_code_0b3882ea21484784956111318de886c7-4"></a>    <span class="k">print</span><span class="p">(</span><span class="s2">"Max Depth: {0}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">max_depth</span><span class="p">))</span>
<a name="rest_code_0b3882ea21484784956111318de886c7-5"></a>    <span class="k">print</span><span class="p">(</span><span class="s2">"Training Accuracy: {0:.2f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">tree</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<a name="rest_code_0b3882ea21484784956111318de886c7-6"></a>    <span class="k">print</span><span class="p">(</span><span class="s2">"Testing Accuracy: {0:.2f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">tree</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
<a name="rest_code_0b3882ea21484784956111318de886c7-7"></a>    <span class="k">print</span><span class="p">()</span>
<a name="rest_code_0b3882ea21484784956111318de886c7-8"></a>    <span class="k">return</span>
<a name="rest_code_0b3882ea21484784956111318de886c7-9"></a>
<a name="rest_code_0b3882ea21484784956111318de886c7-10"></a><span class="n">build_tree</span><span class="p">()</span>
</pre>
<pre class="literal-block">
Max Depth: None
Training Accuracy: 1.00
Testing Accuracy: 0.93
</pre>
<p>It looks like the tree is overfitting the training data. This is because the default tree will have leaf nodes that match each case in the training data set. Limiting the depth of the tree will help with this.</p>
<pre class="code python"><a name="rest_code_e7f764e02f184ec1878f645ed7735953-1"></a><span class="k">for</span> <span class="n">depth</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">):</span>
<a name="rest_code_e7f764e02f184ec1878f645ed7735953-2"></a>    <span class="n">build_tree</span><span class="p">(</span><span class="n">depth</span><span class="p">)</span>
</pre>
<pre class="literal-block">
Max Depth: 1
Training Accuracy: 0.92
Testing Accuracy: 0.90

Max Depth: 2
Training Accuracy: 0.95
Testing Accuracy: 0.92

Max Depth: 3
Training Accuracy: 0.96
Testing Accuracy: 0.92

Max Depth: 4
Training Accuracy: 0.98
Testing Accuracy: 0.91
</pre>
<p>The book was able to get 95% accuracy for the training data, although I don't seem to be able to do better than 92%.</p>
</div>
<div class="section" id="visualizing-the-tree">
<h2>4 Visualizing The Tree</h2>
<pre class="code python"><a name="rest_code_9c96bd96c6044f9f87e5576de6278e3c-1"></a><span class="n">tree</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<a name="rest_code_9c96bd96c6044f9f87e5576de6278e3c-2"></a><span class="n">tree</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<a name="rest_code_9c96bd96c6044f9f87e5576de6278e3c-3"></a><span class="n">export_graphviz</span><span class="p">(</span><span class="n">tree</span><span class="p">,</span> <span class="n">out_file</span><span class="o">=</span><span class="s2">"tree.dot"</span><span class="p">,</span> <span class="n">class_names</span><span class="o">=</span><span class="n">cancer</span><span class="o">.</span><span class="n">target_names</span><span class="p">,</span>
<a name="rest_code_9c96bd96c6044f9f87e5576de6278e3c-4"></a>                <span class="n">feature_names</span><span class="o">=</span><span class="n">cancer</span><span class="o">.</span><span class="n">feature_names</span><span class="p">,</span> <span class="n">impurity</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
<a name="rest_code_9c96bd96c6044f9f87e5576de6278e3c-5"></a>                <span class="n">filled</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre>
<pre class="code python"><a name="rest_code_d97f6191f6a14637810ade52ad381289-1"></a><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">"tree.dot"</span><span class="p">)</span> <span class="k">as</span> <span class="n">reader</span><span class="p">:</span>
<a name="rest_code_d97f6191f6a14637810ade52ad381289-2"></a>    <span class="n">dot_file</span> <span class="o">=</span> <span class="n">reader</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
<a name="rest_code_d97f6191f6a14637810ade52ad381289-3"></a>
<a name="rest_code_d97f6191f6a14637810ade52ad381289-4"></a><span class="n">graphviz</span><span class="o">.</span><span class="n">Source</span><span class="p">(</span><span class="n">dot_file</span><span class="p">,</span> <span class="n">format</span><span class="o">=</span><span class="s2">"png"</span><span class="p">)</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="s2">"tree"</span><span class="p">)</span>
</pre>
<img alt="tree.png" src="posts/decision-tree-classification/tree.png">
</div>
</div>
    </div>
    </article><article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/Naive-Bayes-Classification/" class="u-url">Naive Bayes Classification</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="authors/brunhilde/">Brunhilde</a>
            </span></p>
            <p class="dateline"><a href="posts/Naive-Bayes-Classification/" rel="bookmark"><time class="published dt-published" datetime="2017-07-13T15:45:00-07:00" title="2017-07-13 15:45">2017-07-13 15:45</time></a></p>
        </div>
    </header><div class="e-content entry-content">
    <div>
<div class="section" id="imports">
<h2>1 Imports</h2>
<pre class="code python"><a name="rest_code_1940595754fb4d28949be9631df3b1e5-1"></a><span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">GaussianNB</span>
<a name="rest_code_1940595754fb4d28949be9631df3b1e5-2"></a><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<a name="rest_code_1940595754fb4d28949be9631df3b1e5-3"></a><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_breast_cancer</span>
</pre>
</div>
<div class="section" id="the-data">
<h2>2 The Data</h2>
<pre class="code python"><a name="rest_code_1f17acd6cefc4a19a27dfd6cea873288-1"></a><span class="n">cancer</span> <span class="o">=</span> <span class="n">load_breast_cancer</span><span class="p">()</span>
<a name="rest_code_1f17acd6cefc4a19a27dfd6cea873288-2"></a><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">cancer</span><span class="o">.</span><span class="n">data</span><span class="p">,</span>
<a name="rest_code_1f17acd6cefc4a19a27dfd6cea873288-3"></a>                                                    <span class="n">cancer</span><span class="o">.</span><span class="n">target</span><span class="p">,</span>
<a name="rest_code_1f17acd6cefc4a19a27dfd6cea873288-4"></a>                                                    <span class="n">stratify</span><span class="o">=</span><span class="n">cancer</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
</pre>
</div>
<div class="section" id="the-model">
<h2>3 The Model</h2>
<pre class="code python"><a name="rest_code_efb2bf9acc184b91a5bdb60cbcee59c6-1"></a><span class="n">bayes</span> <span class="o">=</span> <span class="n">GaussianNB</span><span class="p">()</span>
<a name="rest_code_efb2bf9acc184b91a5bdb60cbcee59c6-2"></a><span class="n">bayes</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<a name="rest_code_efb2bf9acc184b91a5bdb60cbcee59c6-3"></a><span class="k">print</span><span class="p">(</span><span class="s2">"Training Accuracy: {0:.2f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">bayes</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<a name="rest_code_efb2bf9acc184b91a5bdb60cbcee59c6-4"></a><span class="k">print</span><span class="p">(</span><span class="s2">"Testing Accuracy: {0:.2f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">bayes</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</pre>
<pre class="literal-block">
Training Accuracy: 0.95
Testing Accuracy: 0.93
</pre>
<p>Naive Bayes works very fast and can handle very large sets of data, but it is called "naive" because it assumes that the features are all independent of each other and so it tends not to generalize as well as some other models. Since it's so efficient it can be used as a baseline to compare with other models.</p>
</div>
</div>
    </div>
    </article><article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/Linear-Classification/" class="u-url">Linear Classification</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="authors/necromuralist/">necromuralist</a>
            </span></p>
            <p class="dateline"><a href="posts/Linear-Classification/" rel="bookmark"><time class="published dt-published" datetime="2017-07-13T12:38:00-07:00" title="2017-07-13 12:38">2017-07-13 12:38</time></a></p>
        </div>
    </header><div class="e-content entry-content">
    <div>
<div class="section" id="imports">
<h2>1 Imports</h2>
<pre class="code python"><a name="rest_code_970274767e9f46bd9121cdd146e6ae9d-1"></a><span class="kn">import</span> <span class="nn">pandas</span>
<a name="rest_code_970274767e9f46bd9121cdd146e6ae9d-2"></a><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<a name="rest_code_970274767e9f46bd9121cdd146e6ae9d-3"></a><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_breast_cancer</span>
<a name="rest_code_970274767e9f46bd9121cdd146e6ae9d-4"></a><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<a name="rest_code_970274767e9f46bd9121cdd146e6ae9d-5"></a><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">LinearSVC</span>
</pre>
</div>
<div class="section" id="the-data">
<h2>2 The Data</h2>
<pre class="code python"><a name="rest_code_1a18f5394cd6423e8e2f779facf46d81-1"></a><span class="n">cancer</span> <span class="o">=</span> <span class="n">load_breast_cancer</span><span class="p">()</span>
<a name="rest_code_1a18f5394cd6423e8e2f779facf46d81-2"></a><span class="k">print</span><span class="p">(</span><span class="n">cancer</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
</pre>
<pre class="literal-block">
dict_keys(['target_names', 'feature_names', 'data', 'DESCR', 'target'])
</pre>
<pre class="code python"><a name="rest_code_d82c9571d55d4377a43be8d1d4a56e92-1"></a><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">cancer</span><span class="o">.</span><span class="n">data</span><span class="p">,</span>
<a name="rest_code_d82c9571d55d4377a43be8d1d4a56e92-2"></a>                                                    <span class="n">cancer</span><span class="o">.</span><span class="n">target</span><span class="p">,</span>
<a name="rest_code_d82c9571d55d4377a43be8d1d4a56e92-3"></a>                                                    <span class="n">stratify</span><span class="o">=</span><span class="n">cancer</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
</pre>
</div>
<div class="section" id="logistic-regression">
<h2>3 Logistic Regression</h2>
<pre class="code python"><a name="rest_code_03539af2bc934e55a2a693303e669bba-1"></a><span class="n">logistic_model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="s2">"l1"</span><span class="p">)</span>
<a name="rest_code_03539af2bc934e55a2a693303e669bba-2"></a><span class="n">logistic_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<a name="rest_code_03539af2bc934e55a2a693303e669bba-3"></a><span class="k">print</span><span class="p">(</span><span class="s2">"Logistic Training Accuracy: {:.2f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">logistic_model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<a name="rest_code_03539af2bc934e55a2a693303e669bba-4"></a><span class="k">print</span><span class="p">(</span><span class="s2">"Logistic Testing Accuracy: {:.2f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">logistic_model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</pre>
<pre class="literal-block">
Logistic Training Accuracy: 0.97
Logistic Testing Accuracy: 0.92
</pre>
<p>Depending on the random seed it sometimes does better on the testing than it does on the training set.</p>
<pre class="code python"><a name="rest_code_f12abe8729d8428288bc0532e88225cf-1"></a><span class="n">coefficients</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">logistic_model</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">index</span><span class="o">=</span><span class="n">cancer</span><span class="o">.</span><span class="n">feature_names</span><span class="p">)</span>
<a name="rest_code_f12abe8729d8428288bc0532e88225cf-2"></a><span class="k">print</span><span class="p">(</span><span class="n">coefficients</span><span class="p">)</span>
</pre>
<pre class="literal-block">
mean radius                2.257338
mean texture               0.058581
mean perimeter            -0.001644
mean area                 -0.009889
mean smoothness            0.000000
mean compactness           0.000000
mean concavity             0.000000
mean concave points        0.000000
mean symmetry              0.000000
mean fractal dimension     0.000000
radius error               0.000000
texture error              2.657975
perimeter error            0.000000
area error                -0.118846
smoothness error           0.000000
compactness error          0.000000
concavity error            0.000000
concave points error       0.000000
symmetry error             0.000000
fractal dimension error    0.000000
worst radius               1.635063
worst texture             -0.412327
worst perimeter           -0.201013
worst area                -0.022727
worst smoothness           0.000000
worst compactness          0.000000
worst concavity           -4.246229
worst concave points       0.000000
worst symmetry             0.000000
worst fractal dimension    0.000000
dtype: float64
</pre>
<pre class="code python"><a name="rest_code_fc502ec0475d459d973c4e0cb65dc0b2-1"></a><span class="n">features</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">cancer</span><span class="o">.</span><span class="n">feature_names</span><span class="p">)</span>
<a name="rest_code_fc502ec0475d459d973c4e0cb65dc0b2-2"></a><span class="n">non_zero</span> <span class="o">=</span> <span class="n">coefficients</span><span class="p">[</span><span class="n">coefficients</span><span class="o">!=</span><span class="mi">0</span><span class="p">]</span>
<a name="rest_code_fc502ec0475d459d973c4e0cb65dc0b2-3"></a><span class="k">print</span><span class="p">(</span><span class="n">non_zero</span><span class="p">)</span>
<a name="rest_code_fc502ec0475d459d973c4e0cb65dc0b2-4"></a><span class="k">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">non_zero</span><span class="p">)</span><span class="o">/</span><span class="n">features</span><span class="p">)</span>
</pre>
<pre class="literal-block">
mean radius        2.257338
mean texture       0.058581
mean perimeter    -0.001644
mean area         -0.009889
texture error      2.657975
area error        -0.118846
worst radius       1.635063
worst texture     -0.412327
worst perimeter   -0.201013
worst area        -0.022727
worst concavity   -4.246229
dtype: float64
0.36666666666666664
</pre>
<p>The model was able to remove 37% of the features.</p>
<pre class="code python"><a name="rest_code_eb5b8c47313d463ea9b801d61f4cbab1-1"></a><span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<a name="rest_code_eb5b8c47313d463ea9b801d61f4cbab1-2"></a><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<a name="rest_code_eb5b8c47313d463ea9b801d61f4cbab1-3"></a><span class="k">print</span><span class="p">(</span><span class="s2">"Training Accuracy: {0:.2f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<a name="rest_code_eb5b8c47313d463ea9b801d61f4cbab1-4"></a><span class="k">print</span><span class="p">(</span><span class="s2">"Testing Accuracy: {0:.2f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</pre>
<pre class="literal-block">
Training Accuracy: 0.98
Testing Accuracy: 0.95
</pre>
<p>Using an <em>L2</em> penalty of 100 improves the accuracy of the model. Increasing "C" means less regularization, so in this case the improvement came from using a more complex model.</p>
</div>
<div class="section" id="support-vector-machine-classification">
<h2>4 Support Vector Machine Classification</h2>
<pre class="code python"><a name="rest_code_61407520433d44e5a4065e4cc003d19d-1"></a><span class="k">for</span> <span class="n">power</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">):</span>
<a name="rest_code_61407520433d44e5a4065e4cc003d19d-2"></a>    <span class="n">penalty</span> <span class="o">=</span> <span class="mi">10</span><span class="o">**</span><span class="n">power</span>
<a name="rest_code_61407520433d44e5a4065e4cc003d19d-3"></a>    <span class="n">svc</span> <span class="o">=</span> <span class="n">LinearSVC</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="n">penalty</span><span class="p">)</span>
<a name="rest_code_61407520433d44e5a4065e4cc003d19d-4"></a>    <span class="n">svc</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<a name="rest_code_61407520433d44e5a4065e4cc003d19d-5"></a>    <span class="k">print</span><span class="p">(</span><span class="s2">"C={}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">penalty</span><span class="p">))</span>
<a name="rest_code_61407520433d44e5a4065e4cc003d19d-6"></a>    <span class="k">print</span><span class="p">(</span><span class="s2">"Training Accuracy: {0:.2f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">svc</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<a name="rest_code_61407520433d44e5a4065e4cc003d19d-7"></a>    <span class="k">print</span><span class="p">(</span><span class="s2">"Testing Accuracy: {0:.2f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">svc</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
<a name="rest_code_61407520433d44e5a4065e4cc003d19d-8"></a>    <span class="k">print</span><span class="p">()</span>
</pre>
<pre class="literal-block">
C=0.0001
Training Accuracy: 0.93
Testing Accuracy: 0.93

C=0.001
Training Accuracy: 0.93
Testing Accuracy: 0.92

C=0.01
Training Accuracy: 0.70
Testing Accuracy: 0.71

C=0.1
Training Accuracy: 0.94
Testing Accuracy: 0.93

C=1
Training Accuracy: 0.92
Testing Accuracy: 0.92

C=10
Training Accuracy: 0.93
Testing Accuracy: 0.94

C=100
Training Accuracy: 0.86
Testing Accuracy: 0.84

C=1000
Training Accuracy: 0.92
Testing Accuracy: 0.92
</pre>
<p>Every time I run this it comes out slightly differently, but it seems like most values do pretty well, there's usually only one or two values of <em>C</em> below 0.92 for the test set.</p>
</div>
<div class="section" id="tuning-the-penalty">
<h2>5 Tuning the Penalty</h2>
<p>The L1 penalty makes use of more of the features so it will generally do better if they are all relevant. The L2 penalty is better for interpreting the important features and will do better if some of the features are in fact not relevant. Unlike <em>alpha</em> for regression, <em>C</em> decreases the regularization as it gets bigger. When searching for the best value it can be useful to search a logarithmic space (e.g. 0.001, 0.01, 0.1, 1, 10, 100)</p>
</div>
</div>
    </div>
    </article><article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/linear-models/" class="u-url">Linear Models</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="authors/brunhilde/">Brunhilde</a>
            </span></p>
            <p class="dateline"><a href="posts/linear-models/" rel="bookmark"><time class="published dt-published" datetime="2017-07-13T12:35:00-07:00" title="2017-07-13 12:35">2017-07-13 12:35</time></a></p>
        </div>
    </header><div class="e-content entry-content">
    <div>
<div class="section" id="linear-regression">
<h2>1 Linear Regression</h2>
<div class="section" id="imports">
<h3>1.1 Imports</h3>
<pre class="code python"><a name="rest_code_c66057eb59c347f3a054e3e9641439d5-1"></a><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">pyplot</span>
<a name="rest_code_c66057eb59c347f3a054e3e9641439d5-2"></a><span class="kn">import</span> <span class="nn">pandas</span>
<a name="rest_code_c66057eb59c347f3a054e3e9641439d5-3"></a><span class="kn">import</span> <span class="nn">seaborn</span>
<a name="rest_code_c66057eb59c347f3a054e3e9641439d5-4"></a><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="p">(</span>
<a name="rest_code_c66057eb59c347f3a054e3e9641439d5-5"></a>    <span class="n">Lasso</span><span class="p">,</span>
<a name="rest_code_c66057eb59c347f3a054e3e9641439d5-6"></a>    <span class="n">LinearRegression</span><span class="p">,</span>
<a name="rest_code_c66057eb59c347f3a054e3e9641439d5-7"></a>    <span class="n">Ridge</span><span class="p">,</span>
<a name="rest_code_c66057eb59c347f3a054e3e9641439d5-8"></a>    <span class="p">)</span>
<a name="rest_code_c66057eb59c347f3a054e3e9641439d5-9"></a><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_boston</span>
<a name="rest_code_c66057eb59c347f3a054e3e9641439d5-10"></a><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
</pre>
<pre class="code python"><a name="rest_code_9b9e10883c2346f7a65b9c09d2d00692-1"></a><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<a name="rest_code_9b9e10883c2346f7a65b9c09d2d00692-2"></a><span class="n">seaborn</span><span class="o">.</span><span class="n">set_style</span><span class="p">(</span><span class="s2">"whitegrid"</span><span class="p">)</span>
</pre>
</div>
<div class="section" id="the-data">
<h3>1.2 The Data</h3>
<p>This is the same data I used for k-nearest neighbors regression.</p>
<pre class="code python"><a name="rest_code_fddc1102638a413798f746b92a942ed6-1"></a><span class="n">boston</span> <span class="o">=</span> <span class="n">load_boston</span><span class="p">()</span>
<a name="rest_code_fddc1102638a413798f746b92a942ed6-2"></a><span class="k">print</span><span class="p">(</span><span class="s2">"Boston data-shape: {0}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">boston</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
</pre>
<pre class="literal-block">
Boston data-shape: (506, 13)
</pre>
<pre class="code python"><a name="rest_code_e383486e740a42a0b71dfd6ed51722c6-1"></a><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">boston</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">boston</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
</pre>
</div>
<div class="section" id="the-model">
<h3>1.3 The Model</h3>
<pre class="code python"><a name="rest_code_86a09c3a4c2e484dbfff5fedd445b4c9-1"></a><span class="n">model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<a name="rest_code_86a09c3a4c2e484dbfff5fedd445b4c9-2"></a><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre>
<pre class="code python"><a name="rest_code_8e2e2af1b17248469d2de34041a6c7f8-1"></a><span class="k">print</span><span class="p">(</span><span class="s2">"coefficients: {0}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">coef_</span><span class="p">))</span>
<a name="rest_code_8e2e2af1b17248469d2de34041a6c7f8-2"></a><span class="k">print</span><span class="p">(</span><span class="s2">"intercept: {0}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">intercept_</span><span class="p">))</span>
</pre>
<pre class="literal-block">
coefficients: [ -5.29188465e-02   3.27516047e-02   5.15495287e-02   1.96191849e+00
  -1.70355026e+01   4.26984342e+00  -4.66261395e-03  -1.24731581e+00
   2.40316945e-01  -1.12757320e-02  -9.67653044e-01   1.07129222e-02
  -4.58665079e-01]
intercept: 31.315219281412134
</pre>
<pre class="code python"><a name="rest_code_a3c84242f43e4940ab5b717ebf01105b-1"></a><span class="n">pollution_index</span> <span class="o">=</span> <span class="mi">4</span>
<a name="rest_code_a3c84242f43e4940ab5b717ebf01105b-2"></a><span class="n">names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"Crime"</span><span class="p">,</span> <span class="s2">"Large Lots"</span><span class="p">,</span> <span class="s2">"Non-Retail Businesses"</span><span class="p">,</span>
<a name="rest_code_a3c84242f43e4940ab5b717ebf01105b-3"></a>         <span class="s2">"Charles River adjacent"</span><span class="p">,</span> <span class="s2">"Nitric Oxide"</span><span class="p">,</span> <span class="s2">"Rooms"</span><span class="p">,</span> <span class="s2">"Old Homes"</span><span class="p">,</span>
<a name="rest_code_a3c84242f43e4940ab5b717ebf01105b-4"></a>         <span class="s2">"Distance to Employment"</span><span class="p">,</span> <span class="s2">"Access to Highways"</span><span class="p">,</span> <span class="s2">"Tax Rate"</span><span class="p">,</span>
<a name="rest_code_a3c84242f43e4940ab5b717ebf01105b-5"></a>         <span class="s2">"Pupil-Teacher Ratio"</span><span class="p">,</span> <span class="s2">"Blacks"</span><span class="p">,</span> <span class="s2">"Lower Status"</span><span class="p">]</span>
<a name="rest_code_a3c84242f43e4940ab5b717ebf01105b-6"></a><span class="n">pandas</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">names</span><span class="p">)</span>
</pre>
<pre class="literal-block">
Crime                     -0.052919
Large Lots                 0.032752
Non-Retail Businesses      0.051550
Charles River adjacent     1.961918
Nitric Oxide             -17.035503
Rooms                      4.269843
Old Homes                 -0.004663
Distance to Employment    -1.247316
Access to Highways         0.240317
Tax Rate                  -0.011276
Pupil-Teacher Ratio       -0.967653
Blacks                     0.010713
Lower Status              -0.458665
dtype: float64
</pre>
<p>The price of homes in Boston is negatively correlated with Crime, Nitric Oxide (pollution), Distance to employment centes, Tax Rate, Pupil:Teacher ratio and the Lower status of the residents, with pollution being the overall largest factor (positive or negative). The most positive factors were the number of rooms the house had and whether the house was adjacent to the Charles River.</p>
<pre class="code python"><a name="rest_code_41bf9429835849e49c02ca60fbe83c17-1"></a><span class="k">print</span><span class="p">(</span><span class="s2">"Training r2: {:.2f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<a name="rest_code_41bf9429835849e49c02ca60fbe83c17-2"></a><span class="k">print</span><span class="p">(</span><span class="s2">"Testing r2: {0:.2f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</pre>
<pre class="literal-block">
Training r2: 0.74
Testing r2: 0.73
</pre>
<p>The training and testing scores were oddly close, suggesting that this model generalizes well.</p>
<pre class="code python"><a name="rest_code_73e11b7d3bb94a71be646af5ac6225f9-1"></a><span class="n">training</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">names</span><span class="p">)</span>
</pre>
<pre class="code python"><a name="rest_code_4b13567b25104ca5a405b957a564af19-1"></a><span class="n">seaborn</span><span class="o">.</span><span class="n">pairplot</span><span class="p">(</span><span class="n">training</span><span class="p">)</span>
</pre>
<img alt="boston_pair_plots.png" src="posts/linear-models/boston_pair_plots.png">
</div>
</div>
<div class="section" id="ridge-regression">
<h2>2 Ridge Regression</h2>
<p>This model uses L2 regression to reduce the size of the coefficients.</p>
<pre class="code python"><a name="rest_code_3f227c46369c41a1b08dd10d3e1d44fc-1"></a><span class="n">ridge</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">()</span>
<a name="rest_code_3f227c46369c41a1b08dd10d3e1d44fc-2"></a><span class="n">ridge</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre>
<pre class="code python"><a name="rest_code_36df71faaae548c789a601db3d6da68d-1"></a><span class="k">print</span><span class="p">(</span><span class="s2">"Training r2: {0:.2f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ridge</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<a name="rest_code_36df71faaae548c789a601db3d6da68d-2"></a><span class="k">print</span><span class="p">(</span><span class="s2">"Testing r2: {:.2f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ridge</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</pre>
<pre class="literal-block">
Training r2: 0.74
Testing r2: 0.72
</pre>
<p>This time the testing did a little worse than without ridge regression.</p>
<pre class="code python"><a name="rest_code_923ef1dcd6b64d0b8645570638c43ad2-1"></a><span class="n">pandas</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">ridge</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">names</span><span class="p">)</span>
</pre>
<pre class="literal-block">
Crime                    -0.048337
Large Lots                0.032897
Non-Retail Businesses     0.016831
Charles River adjacent    1.789245
Nitric Oxide             -8.860668
Rooms                     4.270665
Old Homes                -0.011137
Distance to Employment   -1.125192
Access to Highways        0.224993
Tax Rate                 -0.012211
Pupil-Teacher Ratio      -0.891977
Blacks                    0.010977
Lower Status             -0.471429
dtype: float64
</pre>
<p>Once again pollution and the number of rooms a home had were the biggest influence on the price of the home.</p>
</div>
<div class="section" id="lasso-regression">
<h2>3 Lasso Regression</h2>
<p>This model uses L1 regression to remove the variables that don't influenc the outcome.</p>
<pre class="code python"><a name="rest_code_cb2fd5d389bf44ed888e1c2ff625ceb2-1"></a><span class="n">lasso</span> <span class="o">=</span> <span class="n">Lasso</span><span class="p">()</span>
<a name="rest_code_cb2fd5d389bf44ed888e1c2ff625ceb2-2"></a><span class="n">lasso</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre>
<pre class="code python"><a name="rest_code_3720281222a9402fa2a731c615dc09f0-1"></a><span class="k">print</span><span class="p">(</span><span class="s2">"Training r2: {0:.2f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lasso</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<a name="rest_code_3720281222a9402fa2a731c615dc09f0-2"></a><span class="k">print</span><span class="p">(</span><span class="s2">"Testing r2: {0:.2f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lasso</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</pre>
<pre class="literal-block">
Training r2: 0.67
Testing r2: 0.64
</pre>
<p>The Lasso did worse than the Ridge and ordinary-least-squares models did.</p>
<pre class="code python"><a name="rest_code_46125ba3b7e04ba0ad9b7cbac2ef511d-1"></a><span class="n">coefficients</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">lasso</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">names</span><span class="p">)</span>
<a name="rest_code_46125ba3b7e04ba0ad9b7cbac2ef511d-2"></a><span class="n">coefficients</span><span class="p">[</span><span class="n">coefficients</span><span class="o">==</span><span class="mi">0</span><span class="p">]</span>
</pre>
<pre class="literal-block">
Non-Retail Businesses    -0.0
Charles River adjacent    0.0
Nitric Oxide             -0.0
dtype: float64
</pre>
<p>The Lasso removed Non-Retail Businesses, Charles River adjacency, and pollution, even though the other models decided that pollution was the most important factor.</p>
<p>We can try and do better by using a less aggressive alpha value.</p>
<pre class="code python"><a name="rest_code_8d23811f95d342a5a66b3799ded218ec-1"></a><span class="n">lasso</span> <span class="o">=</span> <span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<a name="rest_code_8d23811f95d342a5a66b3799ded218ec-2"></a><span class="n">lasso</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<a name="rest_code_8d23811f95d342a5a66b3799ded218ec-3"></a><span class="k">print</span><span class="p">(</span><span class="s2">"Training r2: {0:.2f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lasso</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<a name="rest_code_8d23811f95d342a5a66b3799ded218ec-4"></a><span class="k">print</span><span class="p">(</span><span class="s2">"Testing r2: {0:.2f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lasso</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
<a name="rest_code_8d23811f95d342a5a66b3799ded218ec-5"></a><span class="n">coefficients</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">lasso</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">names</span><span class="p">)</span>
<a name="rest_code_8d23811f95d342a5a66b3799ded218ec-6"></a><span class="k">print</span><span class="p">(</span><span class="n">coefficients</span><span class="p">[</span><span class="n">coefficients</span><span class="o">==</span><span class="mi">0</span><span class="p">])</span>
</pre>
<pre class="literal-block">
Training r2: 0.74
Testing r2: 0.73
Series([], dtype: float64)
</pre>
<p>Tuning the alpha can make it perform slightly better than the Ridge regression, but in this case making it aggressive enough to get rid of a column ("Nitric Oxide") makes it perform slightl worse than Ride regression.</p>
<pre class="code python"><a name="rest_code_41884fcd5d9049d8b95d83dd04c71b8c-1"></a><span class="n">training</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">names</span><span class="p">)</span>
<a name="rest_code_41884fcd5d9049d8b95d83dd04c71b8c-2"></a><span class="n">training</span><span class="p">[</span><span class="s2">"price"</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_train</span>
<a name="rest_code_41884fcd5d9049d8b95d83dd04c71b8c-3"></a><span class="n">seaborn</span><span class="o">.</span><span class="n">regplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"Nitric Oxide"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">"price"</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">training</span><span class="p">)</span>
<a name="rest_code_41884fcd5d9049d8b95d83dd04c71b8c-4"></a><span class="n">pyplot</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">"Nitric Oxide"</span><span class="p">)</span>
<a name="rest_code_41884fcd5d9049d8b95d83dd04c71b8c-5"></a><span class="n">pyplot</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">"House Price"</span><span class="p">)</span>
<a name="rest_code_41884fcd5d9049d8b95d83dd04c71b8c-6"></a><span class="n">pyplot</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"Pollution vs House Price"</span><span class="p">)</span>
</pre>
<img alt="pollution_vs_price.png" src="posts/linear-models/pollution_vs_price.png"><p>It appears that there is a linear relationship (although there are what appears to be some outliers).</p>
</div>
</div>
    </div>
    </article><article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/knn-regression/" class="u-url">KNN Regression</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="authors/hades/">hades</a>
            </span></p>
            <p class="dateline"><a href="posts/knn-regression/" rel="bookmark"><time class="published dt-published" datetime="2017-07-09T19:19:00-07:00" title="2017-07-09 19:19">2017-07-09 19:19</time></a></p>
        </div>
    </header><div class="e-content entry-content">
    <div>
<div class="section" id="introduction">
<h2>Introduction</h2>
<p>This will look at using K-Nearest Neighbors for regression. First I'll look at a synthetic data-set and then a dataset that was created to study the effect of polution on the housing prices in Boston.</p>
<div class="section" id="imports">
<h3>Imports</h3>
<pre class="code python"><a name="rest_code_f3a7c0dd361a4a08b117800eb7e104d2-1"></a><span class="kn">from</span> <span class="nn">numba</span> <span class="kn">import</span> <span class="n">jit</span>
<a name="rest_code_f3a7c0dd361a4a08b117800eb7e104d2-2"></a><span class="kn">import</span> <span class="nn">numpy</span>
<a name="rest_code_f3a7c0dd361a4a08b117800eb7e104d2-3"></a><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">pyplot</span>
<a name="rest_code_f3a7c0dd361a4a08b117800eb7e104d2-4"></a><span class="kn">import</span> <span class="nn">seaborn</span>
<a name="rest_code_f3a7c0dd361a4a08b117800eb7e104d2-5"></a><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_regression</span>
<a name="rest_code_f3a7c0dd361a4a08b117800eb7e104d2-6"></a><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<a name="rest_code_f3a7c0dd361a4a08b117800eb7e104d2-7"></a><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsRegressor</span>
</pre>
<pre class="code python"><a name="rest_code_8bf9cdbf73754870b0717adb3e99c041-1"></a><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<a name="rest_code_8bf9cdbf73754870b0717adb3e99c041-2"></a><span class="n">seaborn</span><span class="o">.</span><span class="n">set_style</span><span class="p">(</span><span class="s2">"whitegrid"</span><span class="p">)</span>
</pre>
</div>
<div class="section" id="the-model">
<h3>The Model</h3>
<pre class="code python"><a name="rest_code_7093af183ad74003baab8d01cc154ef0-1"></a><span class="k">def</span> <span class="nf">get_r_squared</span><span class="p">(</span><span class="n">max_neighbors</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">samples</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
<a name="rest_code_7093af183ad74003baab8d01cc154ef0-2"></a>    <span class="n">train_score</span> <span class="o">=</span> <span class="p">[]</span>
<a name="rest_code_7093af183ad74003baab8d01cc154ef0-3"></a>    <span class="n">test_score</span> <span class="o">=</span> <span class="p">[]</span>
<a name="rest_code_7093af183ad74003baab8d01cc154ef0-4"></a>    <span class="n">models</span> <span class="o">=</span> <span class="p">[]</span>
<a name="rest_code_7093af183ad74003baab8d01cc154ef0-5"></a>    <span class="n">inputs</span><span class="p">,</span> <span class="n">values</span> <span class="o">=</span> <span class="n">make_regression</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="n">samples</span><span class="p">)</span>
<a name="rest_code_7093af183ad74003baab8d01cc154ef0-6"></a>    <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">values</span><span class="p">)</span>
<a name="rest_code_7093af183ad74003baab8d01cc154ef0-7"></a>
<a name="rest_code_7093af183ad74003baab8d01cc154ef0-8"></a>    <span class="k">for</span> <span class="n">neighbors</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_neighbors</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
<a name="rest_code_7093af183ad74003baab8d01cc154ef0-9"></a>        <span class="n">model</span> <span class="o">=</span> <span class="n">KNeighborsRegressor</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="n">neighbors</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<a name="rest_code_7093af183ad74003baab8d01cc154ef0-10"></a>        <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<a name="rest_code_7093af183ad74003baab8d01cc154ef0-11"></a>        <span class="n">train_score</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">))</span>
<a name="rest_code_7093af183ad74003baab8d01cc154ef0-12"></a>        <span class="n">test_score</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>
<a name="rest_code_7093af183ad74003baab8d01cc154ef0-13"></a>        <span class="n">models</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<a name="rest_code_7093af183ad74003baab8d01cc154ef0-14"></a>    <span class="k">return</span> <span class="n">train_score</span><span class="p">,</span> <span class="n">test_score</span><span class="p">,</span> <span class="n">models</span>
</pre>
<pre class="code python"><a name="rest_code_cfec011ed1224a96b613a847190db9ba-1"></a><span class="k">def</span> <span class="nf">plot_r_squared</span><span class="p">(</span><span class="n">neighbors</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">samples</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
<a name="rest_code_cfec011ed1224a96b613a847190db9ba-2"></a>    <span class="n">train_score</span><span class="p">,</span> <span class="n">test_score</span><span class="p">,</span> <span class="n">models</span> <span class="o">=</span> <span class="n">get_r_squared</span><span class="p">(</span><span class="n">neighbors</span><span class="p">,</span> <span class="n">samples</span><span class="p">)</span>
<a name="rest_code_cfec011ed1224a96b613a847190db9ba-3"></a>    <span class="n">neighbors</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">neighbors</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
<a name="rest_code_cfec011ed1224a96b613a847190db9ba-4"></a>    <span class="n">pyplot</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">neighbors</span><span class="p">,</span> <span class="n">train_score</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Training $r^2$"</span><span class="p">)</span>
<a name="rest_code_cfec011ed1224a96b613a847190db9ba-5"></a>    <span class="n">pyplot</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">neighbors</span><span class="p">,</span> <span class="n">test_score</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Testing $r^2$"</span><span class="p">)</span>
<a name="rest_code_cfec011ed1224a96b613a847190db9ba-6"></a>    <span class="n">pyplot</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">"Neighbors"</span><span class="p">)</span>
<a name="rest_code_cfec011ed1224a96b613a847190db9ba-7"></a>    <span class="n">pyplot</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">"$r^2$"</span><span class="p">)</span>
<a name="rest_code_cfec011ed1224a96b613a847190db9ba-8"></a>    <span class="n">pyplot</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"KNN Synthetic Data"</span><span class="p">)</span>
<a name="rest_code_cfec011ed1224a96b613a847190db9ba-9"></a>    <span class="n">pyplot</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<a name="rest_code_cfec011ed1224a96b613a847190db9ba-10"></a>    <span class="k">return</span> <span class="n">train_score</span><span class="p">,</span> <span class="n">test_score</span><span class="p">,</span> <span class="n">models</span>
<a name="rest_code_cfec011ed1224a96b613a847190db9ba-11"></a><span class="n">plot_r_squared</span><span class="p">()</span>
</pre>
<img alt="synthetic_r2.png" src="posts/knn-regression/synthetic_r2.png"><p>I originally had it set to a maximum of 10 neighbors, which made it appear that 9 was the peak, but expanding it shows that it was 15. It had a fairly low <span class="math">\(r^2\)</span> score, even at its best. There appears to be more variance in the <tt class="docutils literal">make_regression</tt> function than I had thought. When I ran it earlier the testing score never exceeded the training score and the best <tt class="docutils literal">k</tt> was 12. The actual best score was the same, though.</p>
<pre class="code python"><a name="rest_code_71ad6693d6564e94a8f0a22308345b41-1"></a><span class="k">print</span><span class="p">(</span><span class="s2">"Max r2: {:.2f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">test_score</span><span class="p">)))</span>
</pre>
<pre class="literal-block">
Max r2: 0.47
</pre>
<p>The default for the <tt class="docutils literal">make_regression</tt> function is to create 100 samples (which I mimicked by passing in 100 explicitly). By statistics standards this is a reasonable dataset (I believe 20 samples was the minimum for a long time) but it is very small by machine learning samples. Will it do better if it has a larger sample size?</p>
<pre class="code python"><a name="rest_code_67b331d76cf4419282c2e18d353755b9-1"></a><span class="n">plot_r_squared</span><span class="p">(</span><span class="n">samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
</pre>
<img alt="synthetic_regression_1000.png" src="posts/knn-regression/synthetic_regression_1000.png"><p>It didn't, but maybe because I didn't increase the number of neighbors.</p>
<pre class="code python"><a name="rest_code_7aa7797cbc1a4e8488e1abd796a9e868-1"></a><span class="n">plot_r_squared</span><span class="p">(</span><span class="n">neighbors</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
</pre>
<img alt="synthetic_regression_100_1000.png" src="posts/knn-regression/synthetic_regression_100_1000.png"><p>No, that didn't help, and after re-looking at the plot above I realized that it was getting worse at the end, so I shouldn't have expected that to help. So why does it do worse with more data?</p>
<pre class="code python"><a name="rest_code_06e483e660124d5490519374a4f92612-1"></a><span class="n">train</span><span class="p">,</span> <span class="n">test</span><span class="p">,</span> <span class="n">models</span> <span class="o">=</span> <span class="n">plot_r_squared</span><span class="p">(</span><span class="n">samples</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">neighbors</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</pre>
<img alt="synthetic_10000.png" src="posts/knn-regression/synthetic_10000.png"><p>Having even more data seems to have improved the amount the testing score goes down with the number of neighbors. Maybe there's an ideal neighbors to data points ratio that I'm missing, and too many neighbors means you need more data.</p>
<pre class="code python"><a name="rest_code_015d4ff5ba7d4e06bea7cb943fe8e693-1"></a><span class="nd">@jit</span>
<a name="rest_code_015d4ff5ba7d4e06bea7cb943fe8e693-2"></a><span class="k">def</span> <span class="nf">find_first</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">match</span><span class="p">):</span>
<a name="rest_code_015d4ff5ba7d4e06bea7cb943fe8e693-3"></a>    <span class="sd">"""find the index of the first match</span>
<a name="rest_code_015d4ff5ba7d4e06bea7cb943fe8e693-4"></a>
<a name="rest_code_015d4ff5ba7d4e06bea7cb943fe8e693-5"></a><span class="sd">    Expects a 1-dimensional array or list</span>
<a name="rest_code_015d4ff5ba7d4e06bea7cb943fe8e693-6"></a>
<a name="rest_code_015d4ff5ba7d4e06bea7cb943fe8e693-7"></a><span class="sd">    Args:</span>
<a name="rest_code_015d4ff5ba7d4e06bea7cb943fe8e693-8"></a><span class="sd">     array (numpy.array): thing to search</span>
<a name="rest_code_015d4ff5ba7d4e06bea7cb943fe8e693-9"></a><span class="sd">     match: thing to match</span>
<a name="rest_code_015d4ff5ba7d4e06bea7cb943fe8e693-10"></a>
<a name="rest_code_015d4ff5ba7d4e06bea7cb943fe8e693-11"></a><span class="sd">    Returns:</span>
<a name="rest_code_015d4ff5ba7d4e06bea7cb943fe8e693-12"></a><span class="sd">     int: index of the first match found (or None)</span>
<a name="rest_code_015d4ff5ba7d4e06bea7cb943fe8e693-13"></a><span class="sd">    """</span>
<a name="rest_code_015d4ff5ba7d4e06bea7cb943fe8e693-14"></a>    <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">array</span><span class="p">)):</span>
<a name="rest_code_015d4ff5ba7d4e06bea7cb943fe8e693-15"></a>        <span class="k">if</span> <span class="n">array</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">==</span> <span class="n">match</span><span class="p">:</span>
<a name="rest_code_015d4ff5ba7d4e06bea7cb943fe8e693-16"></a>            <span class="k">return</span> <span class="n">index</span>
<a name="rest_code_015d4ff5ba7d4e06bea7cb943fe8e693-17"></a>    <span class="k">return</span>
</pre>
<pre class="code python"><a name="rest_code_d9b2cc974f0641ca96b8dcf470329c42-1"></a><span class="n">best</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">test</span><span class="p">)</span>
<a name="rest_code_d9b2cc974f0641ca96b8dcf470329c42-2"></a><span class="k">print</span><span class="p">(</span><span class="s2">"Best Test r2: {:.2f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">best</span><span class="p">))</span>
<a name="rest_code_d9b2cc974f0641ca96b8dcf470329c42-3"></a><span class="n">test</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">test</span><span class="p">)</span>
<a name="rest_code_d9b2cc974f0641ca96b8dcf470329c42-4"></a><span class="n">index</span> <span class="o">=</span> <span class="n">find_first</span><span class="p">(</span><span class="n">test</span><span class="p">,</span> <span class="n">best</span><span class="p">)</span>
<a name="rest_code_d9b2cc974f0641ca96b8dcf470329c42-5"></a><span class="k">print</span><span class="p">(</span><span class="s2">"Best Neighbors: {0}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
</pre>
<pre class="literal-block">
Best Test r2: 0.39
Best Neighbors: 18
</pre>
</div>
</div>
<div class="section" id="boston">
<h2>Boston</h2>
<p>This dataset was created to see if there was a correlation between polution and the price of houses in the Boston area.</p>
<div class="section" id="id1">
<h3>Imports</h3>
<pre class="code python"><a name="rest_code_8edc6964f69c4f26b730a23395fe8780-1"></a><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">pyplot</span>
<a name="rest_code_8edc6964f69c4f26b730a23395fe8780-2"></a><span class="kn">import</span> <span class="nn">seaborn</span>
<a name="rest_code_8edc6964f69c4f26b730a23395fe8780-3"></a><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_boston</span>
<a name="rest_code_8edc6964f69c4f26b730a23395fe8780-4"></a><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<a name="rest_code_8edc6964f69c4f26b730a23395fe8780-5"></a><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsRegressor</span>
</pre>
<pre class="code python"><a name="rest_code_58d79df69a734c28964c2df178540437-1"></a><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<a name="rest_code_58d79df69a734c28964c2df178540437-2"></a><span class="n">seaborn</span><span class="o">.</span><span class="n">set_style</span><span class="p">(</span><span class="s2">"whitegrid"</span><span class="p">)</span>
</pre>
</div>
<div class="section" id="the-data">
<h3>The Data</h3>
<pre class="code python"><a name="rest_code_9a2a870f45484b97929a5d4c8939c30b-1"></a><span class="n">boston</span> <span class="o">=</span> <span class="n">load_boston</span><span class="p">()</span>
<a name="rest_code_9a2a870f45484b97929a5d4c8939c30b-2"></a><span class="k">print</span><span class="p">(</span><span class="s2">"Boston data-shape: {0}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">boston</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
</pre>
<pre class="literal-block">
Boston data-shape: (506, 13)
</pre>
<div class="section" id="boston-house-prices-dataset">
<h4>Boston House Prices dataset</h4>
<div class="section" id="notes">
<h5>Notes</h5>
<p>Data Set Characteristics:</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name">
<col class="field-body">
<tbody valign="top">
<tr class="field"><th class="field-name" colspan="2">Number of Instances:</th></tr>
<tr class="field">
<td> </td>
<td class="field-body">506</td>
</tr>
<tr class="field"><th class="field-name" colspan="2">Number of Attributes:</th></tr>
<tr class="field">
<td> </td>
<td class="field-body">13 numeric/categorical predictive</td>
</tr>
<tr class="field">
<th class="field-name">Median Value:</th>
<td class="field-body">(attribute 14) is usually the target</td>
</tr>
<tr class="field"><th class="field-name" colspan="2">Attribute Information (in order):</th></tr>
<tr class="field">
<td> </td>
<td class="field-body"></td>
</tr>
</tbody>
</table>
<ul class="simple">
<li>CRIM     per capita crime rate by town</li>
<li>ZN       proportion of residential land zoned for lots over 25,000 sq.ft.</li>
<li>INDUS    proportion of non-retail business acres per town</li>
<li>CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)</li>
<li>NOX      nitric oxides concentration (parts per 10 million)</li>
<li>RM       average number of rooms per dwelling</li>
<li>AGE      proportion of owner-occupied units built prior to 1940</li>
<li>DIS      weighted distances to five Boston employment centres</li>
<li>RAD      index of accessibility to radial highways</li>
<li>TAX      full-value property-tax rate per $10,000</li>
<li>PTRATIO  pupil-teacher ratio by town</li>
<li>B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town</li>
<li>LSTAT    % lower status of the population</li>
<li>MEDV     Median value of owner-occupied homes in $1000's</li>
</ul>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name">
<col class="field-body">
<tbody valign="top">
<tr class="field"><th class="field-name" colspan="2">Missing Attribute Values:</th></tr>
<tr class="field">
<td> </td>
<td class="field-body">None</td>
</tr>
<tr class="field">
<th class="field-name">Creator:</th>
<td class="field-body">Harrison, D. and Rubinfeld, D.L.</td>
</tr>
</tbody>
</table>
<p>This is a copy of UCI ML housing dataset.
<a class="reference external" href="http://archive.ics.uci.edu/ml/datasets/Housing">http://archive.ics.uci.edu/ml/datasets/Housing</a></p>
<p>This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.</p>
<p>The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic
prices and the demand for clean air', J. Environ. Economics &amp; Management,
vol.5, 81-102, 1978.   Used in Belsley, Kuh &amp; Welsch, 'Regression diagnostics
...', Wiley, 1980.   N.B. Various transformations are used in the table on
pages 244-261 of the latter.</p>
<p>The Boston house-price data has been used in many machine learning papers that address regression
problems.</p>
</div>
<div class="section" id="references">
<h5>References</h5>
<ul class="simple">
<li>Belsley, Kuh &amp; Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.</li>
<li>Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.</li>
<li>many more! (see <a class="reference external" href="http://archive.ics.uci.edu/ml/datasets/Housing">http://archive.ics.uci.edu/ml/datasets/Housing</a>)</li>
</ul>
<pre class="code python"><a name="rest_code_1f69d9892064451abb37c72b9dfb1bd5-1"></a><span class="k">print</span><span class="p">(</span><span class="n">boston</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
</pre>
<pre class="literal-block">
dict_keys(['target', 'feature_names', 'data', 'DESCR'])
</pre>
<p>This time there's no target-names because it is a regression problem instead of a classification problem.</p>
<pre class="code python"><a name="rest_code_84b26c481cbd4c7cb669582b5be2f4db-1"></a><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">boston</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">boston</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
</pre>
</div>
</div>
</div>
<div class="section" id="model-performance">
<h3>Model Performance</h3>
<pre class="code python"><a name="rest_code_81faed7308ce479099731b291abc4df9-1"></a><span class="k">def</span> <span class="nf">get_r_squared</span><span class="p">(</span><span class="n">max_neighbors</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
<a name="rest_code_81faed7308ce479099731b291abc4df9-2"></a>    <span class="n">train_score</span> <span class="o">=</span> <span class="p">[]</span>
<a name="rest_code_81faed7308ce479099731b291abc4df9-3"></a>    <span class="n">test_score</span> <span class="o">=</span> <span class="p">[]</span>
<a name="rest_code_81faed7308ce479099731b291abc4df9-4"></a>    <span class="n">models</span> <span class="o">=</span> <span class="p">[]</span>
<a name="rest_code_81faed7308ce479099731b291abc4df9-5"></a>    <span class="k">for</span> <span class="n">neighbors</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_neighbors</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
<a name="rest_code_81faed7308ce479099731b291abc4df9-6"></a>        <span class="n">model</span> <span class="o">=</span> <span class="n">KNeighborsRegressor</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="n">neighbors</span><span class="p">)</span>
<a name="rest_code_81faed7308ce479099731b291abc4df9-7"></a>        <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<a name="rest_code_81faed7308ce479099731b291abc4df9-8"></a>        <span class="n">train_score</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">))</span>
<a name="rest_code_81faed7308ce479099731b291abc4df9-9"></a>        <span class="n">test_score</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>
<a name="rest_code_81faed7308ce479099731b291abc4df9-10"></a>        <span class="n">models</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<a name="rest_code_81faed7308ce479099731b291abc4df9-11"></a>    <span class="k">return</span> <span class="n">train_score</span><span class="p">,</span> <span class="n">test_score</span><span class="p">,</span> <span class="n">models</span>
</pre>
<pre class="code python"><a name="rest_code_73515114af5640189c5dba6226c20127-1"></a><span class="n">train_score</span><span class="p">,</span> <span class="n">test_score</span><span class="p">,</span> <span class="n">models</span> <span class="o">=</span> <span class="n">get_r_squared</span><span class="p">()</span>
<a name="rest_code_73515114af5640189c5dba6226c20127-2"></a><span class="n">neighbors</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">)</span>
<a name="rest_code_73515114af5640189c5dba6226c20127-3"></a><span class="n">pyplot</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">neighbors</span><span class="p">,</span> <span class="n">train_score</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Training $r^2$"</span><span class="p">)</span>
<a name="rest_code_73515114af5640189c5dba6226c20127-4"></a><span class="n">pyplot</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">neighbors</span><span class="p">,</span> <span class="n">test_score</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Testing $r^2$"</span><span class="p">)</span>
<a name="rest_code_73515114af5640189c5dba6226c20127-5"></a><span class="n">pyplot</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">"Neighbors"</span><span class="p">)</span>
<a name="rest_code_73515114af5640189c5dba6226c20127-6"></a><span class="n">pyplot</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">"$r^2$"</span><span class="p">)</span>
<a name="rest_code_73515114af5640189c5dba6226c20127-7"></a><span class="n">pyplot</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"KNN Boston Housing Prices"</span><span class="p">)</span>
<a name="rest_code_73515114af5640189c5dba6226c20127-8"></a><span class="n">pyplot</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre>
<img alt="boston_r2.png" src="posts/knn-regression/boston_r2.png"><p>The testing score seems to peak at 2 neighbors and then go down from there.</p>
<pre class="code python"><a name="rest_code_fdd9b9067d06438abfca0f53c4cef31b-1"></a><span class="k">print</span><span class="p">(</span><span class="s2">"Training r2 for 2 neigbors: {:.2f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">train_score</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
<a name="rest_code_fdd9b9067d06438abfca0f53c4cef31b-2"></a><span class="k">print</span><span class="p">(</span><span class="s2">"Testing r2 for 2 neighbors: {:.2f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">test_score</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
<a name="rest_code_fdd9b9067d06438abfca0f53c4cef31b-3"></a><span class="k">assert</span> <span class="nb">max</span><span class="p">(</span><span class="n">test_score</span><span class="p">)</span> <span class="o">==</span> <span class="n">test_score</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</pre>
<pre class="literal-block">
Training r2 for 2 neigbors: 0.84
Testing r2 for 2 neighbors: 0.63
</pre>
<p>In this case the K-Nearest Neighbors didn't seem to do as well with regression as it did with classification.</p>
</div>
</div>
</div>
    </div>
    </article><article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/knn-classification/" class="u-url">KNN Classification</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="authors/hades/">hades</a>
            </span></p>
            <p class="dateline"><a href="posts/knn-classification/" rel="bookmark"><time class="published dt-published" datetime="2017-07-09T19:00:00-07:00" title="2017-07-09 19:00">2017-07-09 19:00</time></a></p>
        </div>
    </header><div class="e-content entry-content">
    <div>
<div class="section" id="introduction">
<h2>1 Introduction</h2>
<p>This looks at the performance of the K-Nearest Neighbors for classification. K-Nearest Neighbors works by finding the <tt class="docutils literal">k</tt> (count) of neighbors that are closest to the data-point and classifying the point using the majority vote of those points. I'm going to use the default distance measurement of Euclidean distance. Fitting in this case means memorizing all the data so you can use it for predictions and then doing the calculations when you need to make a prediction. This makes it memory-intensive and slower when it's used to make predictions, so it's useful as a baseline, but not in production.</p>
</div>
<div class="section" id="synthetic">
<h2>2 Synthetic</h2>
<p>I'll start with a synthetic data set created by sklean. I'll make it the same shape as the Breast Cancer case that I'll look at later.</p>
<div class="section" id="imports">
<h3>2.1 Imports</h3>
<pre class="code python"><a name="rest_code_a1c1040c0e724b88a3d06a2d14f1fe43-1"></a><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>
<a name="rest_code_a1c1040c0e724b88a3d06a2d14f1fe43-2"></a><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<a name="rest_code_a1c1040c0e724b88a3d06a2d14f1fe43-3"></a><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<a name="rest_code_a1c1040c0e724b88a3d06a2d14f1fe43-4"></a><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">pyplot</span>
<a name="rest_code_a1c1040c0e724b88a3d06a2d14f1fe43-5"></a><span class="kn">import</span> <span class="nn">seaborn</span>
</pre>
<pre class="code python"><a name="rest_code_db11b95284124f33bdd67644d74c4814-1"></a><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<a name="rest_code_db11b95284124f33bdd67644d74c4814-2"></a><span class="n">seaborn</span><span class="o">.</span><span class="n">set_style</span><span class="p">(</span><span class="s2">"whitegrid"</span><span class="p">)</span>
</pre>
</div>
<div class="section" id="the-data">
<h3>2.2 The Data</h3>
<pre class="code python"><a name="rest_code_981521ba50e64f4b8160f8566e19dd76-1"></a><span class="n">total</span> <span class="o">=</span> <span class="mi">569</span>
<a name="rest_code_981521ba50e64f4b8160f8566e19dd76-2"></a><span class="n">positive_fraction</span> <span class="o">=</span> <span class="mi">212</span><span class="o">/</span><span class="n">total</span>
<a name="rest_code_981521ba50e64f4b8160f8566e19dd76-3"></a><span class="n">negative_fraction</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">positive_fraction</span>
<a name="rest_code_981521ba50e64f4b8160f8566e19dd76-4"></a><span class="n">inputs</span><span class="p">,</span> <span class="n">classifications</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="n">total</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span>
<a name="rest_code_981521ba50e64f4b8160f8566e19dd76-5"></a>                                              <span class="n">weights</span><span class="o">=</span><span class="p">[</span><span class="n">positive_fraction</span><span class="p">,</span>
<a name="rest_code_981521ba50e64f4b8160f8566e19dd76-6"></a>                                                       <span class="n">negative_fraction</span><span class="p">])</span>
<a name="rest_code_981521ba50e64f4b8160f8566e19dd76-7"></a><span class="k">print</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<a name="rest_code_981521ba50e64f4b8160f8566e19dd76-8"></a><span class="k">print</span><span class="p">(</span><span class="n">classifications</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre>
<pre class="literal-block">
(569, 30)
(569,)
</pre>
<pre class="code python"><a name="rest_code_9db01737ce0c4a7e912cc0e261396322-1"></a><span class="n">positive</span> <span class="o">=</span> <span class="n">classifications</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<a name="rest_code_9db01737ce0c4a7e912cc0e261396322-2"></a><span class="k">print</span><span class="p">(</span><span class="s2">"Positives: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">positive</span><span class="p">))</span>
<a name="rest_code_9db01737ce0c4a7e912cc0e261396322-3"></a><span class="k">print</span><span class="p">(</span><span class="s2">"Negatives: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">classifications</span><span class="o">.</span><span class="n">size</span> <span class="o">-</span> <span class="n">positive</span><span class="p">))</span>
</pre>
<pre class="literal-block">
Positives: 355
Negatives: 214
</pre>
<pre class="code python"><a name="rest_code_92324f6ebaaa499096abf38849f1e27c-1"></a><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">classifications</span><span class="p">)</span>
</pre>
</div>
<div class="section" id="the-model">
<h3>2.3 The model</h3>
<pre class="code python"><a name="rest_code_ed6b7fa147c84cff99747c631fa3ff63-1"></a><span class="n">model</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">()</span>
</pre>
<pre class="code python"><a name="rest_code_e8079dedbe95417da6764126a7f4e1f0-1"></a><span class="k">def</span> <span class="nf">get_accuracies</span><span class="p">(</span><span class="n">max_neighbors</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
<a name="rest_code_e8079dedbe95417da6764126a7f4e1f0-2"></a>    <span class="n">train_accuracies</span> <span class="o">=</span> <span class="p">[]</span>
<a name="rest_code_e8079dedbe95417da6764126a7f4e1f0-3"></a>    <span class="n">test_accuracies</span> <span class="o">=</span> <span class="p">[]</span>
<a name="rest_code_e8079dedbe95417da6764126a7f4e1f0-4"></a>    <span class="k">for</span> <span class="n">neighbors</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span>  <span class="n">max_neighbors</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
<a name="rest_code_e8079dedbe95417da6764126a7f4e1f0-5"></a>        <span class="n">classifier</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="n">neighbors</span><span class="p">)</span>
<a name="rest_code_e8079dedbe95417da6764126a7f4e1f0-6"></a>        <span class="n">classifier</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<a name="rest_code_e8079dedbe95417da6764126a7f4e1f0-7"></a>        <span class="n">train_accuracies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">classifier</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">))</span>
<a name="rest_code_e8079dedbe95417da6764126a7f4e1f0-8"></a>        <span class="n">test_accuracies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">classifier</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>
<a name="rest_code_e8079dedbe95417da6764126a7f4e1f0-9"></a>    <span class="k">return</span> <span class="n">train_accuracies</span><span class="p">,</span> <span class="n">test_accuracies</span>
</pre>
<pre class="code python"><a name="rest_code_1d6c96f632a54c489a9c1892efd0dced-1"></a><span class="n">training_accuracies</span><span class="p">,</span> <span class="n">testing_accuracies</span> <span class="o">=</span> <span class="n">get_accuracies</span><span class="p">()</span>
</pre>
<pre class="code python"><a name="rest_code_eb47a0eed5d844d68198ed7d8906b26c-1"></a><span class="n">neighbors</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">)</span>
<a name="rest_code_eb47a0eed5d844d68198ed7d8906b26c-2"></a><span class="n">pyplot</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">neighbors</span><span class="p">,</span> <span class="n">training_accuracies</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Training Accuracy"</span><span class="p">)</span>
<a name="rest_code_eb47a0eed5d844d68198ed7d8906b26c-3"></a><span class="n">pyplot</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">neighbors</span><span class="p">,</span> <span class="n">testing_accuracies</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Testing Accuracy"</span><span class="p">)</span>
<a name="rest_code_eb47a0eed5d844d68198ed7d8906b26c-4"></a><span class="n">pyplot</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">"Accuracy"</span><span class="p">)</span>
<a name="rest_code_eb47a0eed5d844d68198ed7d8906b26c-5"></a><span class="n">pyplot</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">"Neighbors"</span><span class="p">)</span>
<a name="rest_code_eb47a0eed5d844d68198ed7d8906b26c-6"></a><span class="n">pyplot</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"KNN Cancer Accuracy"</span><span class="p">)</span>
<a name="rest_code_eb47a0eed5d844d68198ed7d8906b26c-7"></a><span class="n">pyplot</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre>
<img alt="knn_synthetic_accuracy.png" src="posts/knn-classification/knn_synthetic_accuracy.png"><p>At <em>k=1</em>, the training set does perfectly while the test set does okay, but not as well as it does at <em>k=9</em>, what appears to be the best value.</p>
</div>
</div>
<div class="section" id="breast-cancer">
<h2>3 Breast Cancer</h2>
<div class="section" id="id1">
<h3>3.1 Imports</h3>
<pre class="code python"><a name="rest_code_9b824fb9a80e4567b00d8aa2c68353a3-1"></a><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">pyplot</span>
<a name="rest_code_9b824fb9a80e4567b00d8aa2c68353a3-2"></a><span class="kn">import</span> <span class="nn">seaborn</span>
<a name="rest_code_9b824fb9a80e4567b00d8aa2c68353a3-3"></a><span class="kn">import</span> <span class="nn">pandas</span>
<a name="rest_code_9b824fb9a80e4567b00d8aa2c68353a3-4"></a><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<a name="rest_code_9b824fb9a80e4567b00d8aa2c68353a3-5"></a><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_breast_cancer</span>
<a name="rest_code_9b824fb9a80e4567b00d8aa2c68353a3-6"></a><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
</pre>
<pre class="code python"><a name="rest_code_36403ccc809c493f818820931c213b34-1"></a><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<a name="rest_code_36403ccc809c493f818820931c213b34-2"></a><span class="n">seaborn</span><span class="o">.</span><span class="n">set_style</span><span class="p">(</span><span class="s2">"whitegrid"</span><span class="p">)</span>
</pre>
</div>
<div class="section" id="the-dataset">
<h3>3.2 The Dataset</h3>
<pre class="code python"><a name="rest_code_50945129762c415f9cc6e53d8c26ba08-1"></a><span class="n">cancer</span> <span class="o">=</span> <span class="n">load_breast_cancer</span><span class="p">()</span>
<a name="rest_code_50945129762c415f9cc6e53d8c26ba08-2"></a><span class="k">print</span><span class="p">(</span><span class="s2">"Keys in the cancer bunch: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">","</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">cancer</span><span class="o">.</span><span class="n">keys</span><span class="p">())))</span>
<a name="rest_code_50945129762c415f9cc6e53d8c26ba08-3"></a><span class="k">print</span><span class="p">(</span><span class="s2">"Training Data Shape: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">cancer</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<a name="rest_code_50945129762c415f9cc6e53d8c26ba08-4"></a><span class="k">print</span><span class="p">(</span><span class="s2">"Target Names: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s1">','</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">cancer</span><span class="o">.</span><span class="n">target_names</span><span class="p">)))</span>
</pre>
<pre class="literal-block">
Keys in the cancer bunch: feature_names,target_names,target,data,DESCR
Training Data Shape: (569, 30)
Target Names: malignant,benign
</pre>
<p>This is from the description.</p>
<dl class="docutils">
<dt>Data Set Characteristics:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name">
<col class="field-body">
<tbody valign="top">
<tr class="field"><th class="field-name" colspan="2">Number of Instances:</th></tr>
<tr class="field">
<td> </td>
<td class="field-body">569</td>
</tr>
</tbody>
</table></dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name">
<col class="field-body">
<tbody valign="top">
<tr class="field"><th class="field-name" colspan="2">Number of Attributes:</th></tr>
<tr class="field">
<td> </td>
<td class="field-body">30 numeric, predictive attributes and the class</td>
</tr>
<tr class="field"><th class="field-name" colspan="2">Attribute Information:</th></tr>
<tr class="field">
<td> </td>
<td class="field-body"></td>
</tr>
</tbody>
</table>
<ul class="simple">
<li>radius (mean of distances from center to points on the perimeter)</li>
<li>texture (standard deviation of gray-scale values)</li>
<li>perimeter</li>
<li>area</li>
<li>smoothness (local variation in radius lengths)</li>
<li>compactness (perimeter^2 / area - 1.0)</li>
<li>concavity (severity of concave portions of the contour)</li>
<li>concave points (number of concave portions of the contour)</li>
<li>symmetry</li>
<li>fractal dimension ("coastline approximation" - 1)</li>
</ul>
<p>The mean, standard error, and "worst" or largest (mean of the three
largest values) of these features were computed for each image,
resulting in 30 features.  For instance, field 3 is Mean Radius, field
13 is Radius SE, field 23 is Worst Radius.</p>
<ul class="simple">
<li>class:<ul>
<li>WDBC-Malignant</li>
<li>WDBC-Benign</li>
</ul>
</li>
</ul>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name">
<col class="field-body">
<tbody valign="top">
<tr class="field"><th class="field-name" colspan="2">Missing Attribute Values:</th></tr>
<tr class="field">
<td> </td>
<td class="field-body">None</td>
</tr>
<tr class="field"><th class="field-name" colspan="2">Class Distribution:</th></tr>
<tr class="field">
<td> </td>
<td class="field-body">212 - Malignant, 357 - Benign</td>
</tr>
<tr class="field">
<th class="field-name">Creator:</th>
<td class="field-body">Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian</td>
</tr>
<tr class="field">
<th class="field-name">Donor:</th>
<td class="field-body">Nick Street</td>
</tr>
<tr class="field">
<th class="field-name">Date:</th>
<td class="field-body">November, 1995</td>
</tr>
</tbody>
</table>
<p>This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.
<a class="reference external" href="https://goo.gl/U2Uwz2">https://goo.gl/U2Uwz2</a></p>
<p>Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass.  They describe characteristics of the cell nuclei present in the image.</p>
<p>Separating plane described above was obtained using Multisurface Method-Tree (MSM-T) [K. P. Bennett, "Decision Tree Construction Via Linear Programming." Proceedings of the 4th Midwest Artificial Intelligence and Cognitive Science Society, pp. 97-101, 1992], a classification method which uses linear programming to construct a decision tree.  Relevant features were selected using an exhaustive search in the space of 1-4 features and 1-3 separating planes.</p>
<p>The actual linear program used to obtain the separating plane in the 3-dimensional space is that described in:
[K. P. Bennett and O. L. Mangasarian: "Robust Linear Programming Discrimination of Two Linearly Inseparable Sets",
Optimization Methods and Software 1, 1992, 23-34].</p>
<p>This database is also available through the UW CS ftp server:</p>
<p>ftp ftp.cs.wisc.edu
cd math-prog/cpo-dataset/machine-learn/WDBC/</p>
</div>
<div class="section" id="references">
<h3>References</h3>
<ul class="simple">
<li>W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction
for breast tumor diagnosis. IS&amp;T/SPIE 1993 International Symposium on
Electronic Imaging: Science and Technology, volume 1905, pages 861-870,
San Jose, CA, 1993.</li>
<li>O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and
prognosis via linear programming. Operations Research, 43(4), pages 570-577,
July-August 1995.</li>
<li>W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques
to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994)
163-171.</li>
</ul>
</div>
<div class="section" id="loading-the-data">
<h3>Loading the Data</h3>
<pre class="code python"><a name="rest_code_cb08e2407c3948cba1c9294f4b8c5a08-1"></a><span class="n">target</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">cancer</span><span class="o">.</span><span class="n">target</span><span class="p">))</span>
<a name="rest_code_cb08e2407c3948cba1c9294f4b8c5a08-2"></a><span class="n">target_map</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">cancer</span><span class="o">.</span><span class="n">target_names</span><span class="p">)),</span> <span class="n">cancer</span><span class="o">.</span><span class="n">target_names</span><span class="p">))</span>
<a name="rest_code_cb08e2407c3948cba1c9294f4b8c5a08-3"></a><span class="n">target</span><span class="p">[</span><span class="s1">'name'</span><span class="p">]</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">entry</span><span class="p">:</span> <span class="n">target_map</span><span class="p">[</span><span class="n">entry</span><span class="p">])</span>
<a name="rest_code_cb08e2407c3948cba1c9294f4b8c5a08-4"></a><span class="k">print</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">value_counts</span><span class="p">())</span>
</pre>
<pre class="literal-block">
benign       357
malignant    212
Name: name, dtype: int64
</pre>
</div>
<div class="section" id="splitting-the-data">
<h3>3.3 Splitting the Data</h3>
<pre class="code python"><a name="rest_code_4e58f4da8ee241f98d1745d839877373-1"></a><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">cancer</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">cancer</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">cancer</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
<a name="rest_code_4e58f4da8ee241f98d1745d839877373-2"></a><span class="k">print</span><span class="p">(</span><span class="s2">"Trainining percent: {0:.2f} %"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="mi">100</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">cancer</span><span class="o">.</span><span class="n">target</span><span class="p">)))</span>
<a name="rest_code_4e58f4da8ee241f98d1745d839877373-3"></a><span class="k">print</span><span class="p">(</span><span class="s2">"Testing percent: {0:.2f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="mi">100</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_test</span><span class="p">)</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">cancer</span><span class="o">.</span><span class="n">target</span><span class="p">)))</span>
</pre>
<pre class="literal-block">
Trainining percent: 74.87 %
Testing percent: 25.13
</pre>
</div>
<div class="section" id="model-performance">
<h3>3.4 Model Performance</h3>
<pre class="code python"><a name="rest_code_6433bd72ca7b49e48af1bc7bdb06f7aa-1"></a><span class="k">def</span> <span class="nf">get_accuracies</span><span class="p">(</span><span class="n">max_neighbors</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
<a name="rest_code_6433bd72ca7b49e48af1bc7bdb06f7aa-2"></a>    <span class="n">train_accuracies</span> <span class="o">=</span> <span class="p">[]</span>
<a name="rest_code_6433bd72ca7b49e48af1bc7bdb06f7aa-3"></a>    <span class="n">test_accuracies</span> <span class="o">=</span> <span class="p">[]</span>
<a name="rest_code_6433bd72ca7b49e48af1bc7bdb06f7aa-4"></a>    <span class="k">for</span> <span class="n">neighbors</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span>  <span class="n">max_neighbors</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
<a name="rest_code_6433bd72ca7b49e48af1bc7bdb06f7aa-5"></a>        <span class="n">classifier</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="n">neighbors</span><span class="p">)</span>
<a name="rest_code_6433bd72ca7b49e48af1bc7bdb06f7aa-6"></a>        <span class="n">classifier</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<a name="rest_code_6433bd72ca7b49e48af1bc7bdb06f7aa-7"></a>        <span class="n">train_accuracies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">classifier</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">))</span>
<a name="rest_code_6433bd72ca7b49e48af1bc7bdb06f7aa-8"></a>        <span class="n">test_accuracies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">classifier</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>
<a name="rest_code_6433bd72ca7b49e48af1bc7bdb06f7aa-9"></a>    <span class="k">return</span> <span class="n">train_accuracies</span><span class="p">,</span> <span class="n">test_accuracies</span>
</pre>
<pre class="code python"><a name="rest_code_88627ec7bca34a24987800cddba3e2f3-1"></a><span class="n">training_accuracies</span><span class="p">,</span> <span class="n">testing_accuracies</span> <span class="o">=</span> <span class="n">get_accuracies</span><span class="p">()</span>
</pre>
<pre class="code python"><a name="rest_code_fe37e3e8d6ac4a6385591a7de6a6d6f7-1"></a><span class="n">neighbors</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">)</span>
<a name="rest_code_fe37e3e8d6ac4a6385591a7de6a6d6f7-2"></a><span class="n">pyplot</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">neighbors</span><span class="p">,</span> <span class="n">training_accuracies</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Training Accuracy"</span><span class="p">)</span>
<a name="rest_code_fe37e3e8d6ac4a6385591a7de6a6d6f7-3"></a><span class="n">pyplot</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">neighbors</span><span class="p">,</span> <span class="n">testing_accuracies</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Testing Accuracy"</span><span class="p">)</span>
<a name="rest_code_fe37e3e8d6ac4a6385591a7de6a6d6f7-4"></a><span class="n">pyplot</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">"Accuracy"</span><span class="p">)</span>
<a name="rest_code_fe37e3e8d6ac4a6385591a7de6a6d6f7-5"></a><span class="n">pyplot</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">"Neighbors"</span><span class="p">)</span>
<a name="rest_code_fe37e3e8d6ac4a6385591a7de6a6d6f7-6"></a><span class="n">pyplot</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"KNN Cancer Accuracy"</span><span class="p">)</span>
<a name="rest_code_fe37e3e8d6ac4a6385591a7de6a6d6f7-7"></a><span class="n">pyplot</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre>
<img alt="knn_cancer_accuracy.png" src="posts/knn-classification/knn_cancer_accuracy.png"><p>It looks like five neighbors would be what you'd want.</p>
<pre class="code python"><a name="rest_code_d45f9d3e10c942ccabcb3abeb77bc7a2-1"></a><span class="k">print</span><span class="p">(</span><span class="s2">"Minimum test accuracy (n=1): {:.2f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">testing_accuracies</span><span class="p">)))</span>
<a name="rest_code_d45f9d3e10c942ccabcb3abeb77bc7a2-2"></a><span class="k">print</span><span class="p">(</span><span class="s2">"Maximum test accuracy (n=5): {:.2f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">testing_accuracies</span><span class="p">)))</span>
<a name="rest_code_d45f9d3e10c942ccabcb3abeb77bc7a2-3"></a><span class="k">assert</span> <span class="nb">max</span><span class="p">(</span><span class="n">testing_accuracies</span> <span class="o">==</span> <span class="n">testing_accuracies</span><span class="p">[</span><span class="mi">4</span><span class="p">])</span>
</pre>
<pre class="literal-block">
Minimum test accuracy (n=1): 0.90
Maximum test accuracy (n=5): 0.92
</pre>
<p>The original paper that used this data-set got a cross-validation error-rate of 3%, but it sounds like they didn't split the data into training and testing sets (I'll have to re-read the paper to be sure).</p>
</div>
</div>
</div>
    </div>
    </article>
</div>







        </div>
        <!--End of body content-->

        <footer id="footer">
            Contents © 2018         <a href="mailto:necromuralist@gmail.com">necromuralist</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         <a rel="license" href="http://creativecommons.org/licenses/by/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by/4.0/88x31.png"></a><br>This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.
            
        </footer>
</div>
</div>


            <script src="assets/js/all-nocdn.js"></script><script>$('a.image-reference:not(.islink) img:not(.islink)').parent().colorbox({rel:"gal",maxWidth:"100%",maxHeight:"100%",scalePhotos:true});</script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates -->
</body>
</html>
