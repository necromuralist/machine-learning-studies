#+TITLE: Linear Models

* Linear Regression
** Imports
#+BEGIN_SRC ipython :session boston :results none
import pandas
import seaborn
from sklearn.linear_model import (
    LinearRegression,
    Ridge,
    )
from sklearn.datasets import load_boston
from sklearn.model_selection import train_test_split
#+END_SRC

#+BEGIN_SRC ipython :session boston :results none
%matplotlib inline
seaborn.set_style("whitegrid")
#+END_SRC
** The Data
   This is the same data I used for k-nearest neighbors regression.
#+BEGIN_SRC ipython :session boston :results output
boston = load_boston()
print("Boston data-shape: {0}".format(boston.data.shape))
#+END_SRC

#+RESULTS:
: Boston data-shape: (506, 13)

#+BEGIN_SRC ipython :session boston :results none
X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target)
#+END_SRC

** The Model
#+BEGIN_SRC ipython :session boston :results none
model = LinearRegression()
model.fit(X_train, y_train)
#+END_SRC

#+BEGIN_SRC ipython :session boston :results output
print("coefficients: {0}".format(model.coef_))
print("intercept: {0}".format(model.intercept_))
#+END_SRC

#+RESULTS:
: coefficients: [ -1.22346009e-01   6.26327008e-02   4.86207764e-02   2.63139124e+00
:   -2.15499623e+01   3.01081119e+00   1.38722795e-02  -1.59021844e+00
:    3.39903362e-01  -1.35682064e-02  -9.99762482e-01   8.08257897e-03
:   -5.90947689e-01]
: intercept: 44.99902526137056

#+BEGIN_SRC ipython :session boston
names = ["Crime", "Large Lots", "Non-Retail Businesses",
         "Charles River adjacent", "Nitric Oxide", "Rooms", "Old Homes",
         "Distance to Employment", "Access to Highways", "Tax Rate",
         "Pupil-Teacher Ratio", "Blacks", "Lower Status"]
pandas.Series(model.coef_, index=names)
#+END_SRC

#+RESULTS:
#+begin_example
Crime                     -0.122346
Large Lots                 0.062633
Non-Retail Businesses      0.048621
Charles River adjacent     2.631391
Nitric Oxide             -21.549962
Rooms                      3.010811
Old Homes                  0.013872
Distance to Employment    -1.590218
Access to Highways         0.339903
Tax Rate                  -0.013568
Pupil-Teacher Ratio       -0.999762
Blacks                     0.008083
Lower Status              -0.590948
dtype: float64
#+end_example

The price of homes in Boston is negatively correlated with Crime, Nitric Oxide (pollution), Distance to employment centes, Tax Rate, Pupil:Teacher ratio and the Lower status of the residents, with polution being the overall largest factor (positive or negative). The most positive factors were the number of rooms the house had and whether the house was adjacent to the Charles River.

#+BEGIN_SRC ipython :session boston :results output
print("Training r2: {:.2f}".format(model.score(X_train, y_train)))
print("Testing r2: {0:.2f}".format(model.score(X_test, y_test)))
#+END_SRC

#+RESULTS:
: Training r2: 0.73
: Testing r2: 0.77

The model did inexplicably better on the test-set than it did on the training set. This seems like it might be a mistake... Or the training data had strange outliers that threw it off.

#+BEGIN_SRC ipython :session boston :results none
training = pandas.DataFrame(X_train, columns=names)
#+END_SRC

#+BEGIN_SRC ipython :session boston :file /tmp/boston_pair_plots.png :exports both
seaborn.pairplot(training)
#+END_SRC

#+RESULTS:
[[file:/tmp/boston_pair_plots.png]]

* Ridge Regression

This model uses L2 regression to reduce the size of the coefficients.

#+BEGIN_SRC ipython :session boston :results none
ridge = Ridge()
ridge.fit(X_train, y_train)
#+END_SRC

#+BEGIN_SRC ipython :session boston :results output
print("Training r2: {0:.2f}".format(ridge.score(X_train, y_train)))
print("Testing r2: {:.2f}".format(ridge.score(X_test, y_test)))
#+END_SRC

#+RESULTS:
: Training r2: 0.72
: Testing r2: 0.78

This time the testing set did even better while the the training set did slightly worse. Because regularization reduces the complexity of the model, it does worse on the training data (reducing overfitting) but better on the testing data (more generalization).
