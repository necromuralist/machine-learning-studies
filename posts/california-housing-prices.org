#+BEGIN_COMMENT
.. title: California Housing Prices
.. slug: california-housing-prices
.. date: 2018-07-30 16:54:39 UTC-07:00
.. tags: regression basics
.. category: regression
.. link: 
.. description: An introductory-level regression using California housing data.
.. type: text
#+END_COMMENT
#+OPTIONS: ^:nil
#+TOC: headlines 1
* Introduction
  This is an introductory regression problem that uses California housing data from the 1990 census. There's a description of the original data [[https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.htm][here]], but we're using a slightly altered dataset that's [[https://github.com/ageron/handson-ml/tree/master/datasets/housing][on github]] (and appears to be mirrored [[https://www.kaggle.com/camnugent/california-housing-prices][on kaggle]]). The problem here is to create a model that will predict the median housing value for a census block group (called "district" in the dataset) given the other attributes.

* Imports
  These are the dependencies for this problem.

#+BEGIN_SRC ipython :session housing :results none
# python standard library
import os
import tarfile
import warnings
warnings.filterwarnings("ignore", message="numpy.dtype size changed")
from http import HTTPStatus

# from pypi
import matplotlib
import pandas
import requests
import seaborn
from tabulate import tabulate
#+END_SRC

#+BEGIN_SRC ipython :session housing :results none :exports none
% matplotlib inline
#+END_SRC
* Constants
  These are convenience holders for strings and other constants so they don't get scattered all over the place.

#+BEGIN_SRC ipython :session housing :results none
class Data:
    source_slug = "../data/california-housing-prices/"
    target_slug = "../data_temp/california-housing-prices/"
    url = "https://github.com/ageron/handson-ml/raw/master/datasets/housing/housing.tgz"
    source = source_slug + "housing.tgz"
    target = target_slug + "housing.csv"
    chunk_size = 128
#+END_SRC
* The Data
  We'll grab the data from github, extract it (it's a =tgz= compressed tarfile), then make a pandas data frame from it.
** Downloading and uncompressing the data
#+BEGIN_SRC ipython :session housing :results none
def get_data():
    """Gets the data from github and uncompresses it"""
    if os.path.exists(Data.target):
        return

    os.makedirs(Data.target_slug, exist_ok=True)
    os.makedirs(Data.source_slug, exist_ok=True)
    response = requests.get(Data.url, stream=True)
    assert response.status_code == HTTPStatus.OK
    with open(Data.source, "wb") as writer:
        for chunk in response.iter_content(chunk_size=Data.chunk_size):
            writer.write(chunk)
    assert os.path.exists(Data.source)
    compressed = tarfile.open(Data.source)
    compressed.extractall(Data.target_slug)
    compressed.close()
    assert os.path.exists(Data.target)
    return
#+END_SRC


#+BEGIN_SRC ipython :session housing :results output raw :exports results
print("Contents of {}:".format(Data.target_slug))
if not os.path.exists(Data.target):
    get_data()

for name in os.listdir(Data.target_slug):
    print("   - {}".format(name))
#+END_SRC

#+RESULTS:
Contents of ../data_temp/california-housing-prices/:
   - housing.csv
** Building the dataframe

#+BEGIN_SRC ipython :session housing :results none
housing = pandas.read_csv(Data.target)
#+END_SRC

#+BEGIN_SRC ipython :session housing :results output :exports results
print(housing.info())
#+END_SRC

#+RESULTS:
#+begin_example
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 20640 entries, 0 to 20639
Data columns (total 10 columns):
longitude             20640 non-null float64
latitude              20640 non-null float64
housing_median_age    20640 non-null float64
total_rooms           20640 non-null float64
total_bedrooms        20433 non-null float64
population            20640 non-null float64
households            20640 non-null float64
median_income         20640 non-null float64
median_house_value    20640 non-null float64
ocean_proximity       20640 non-null object
dtypes: float64(9), object(1)
memory usage: 1.6+ MB
None
#+end_example

#+BEGIN_SRC ipython :session housing :results output raw :exports results
print("|Rows | Columns|")
print("|-+-|")
print("| {} | {} |".format(*housing.shape))
#+END_SRC

#+RESULTS:
|  Rows | Columns |
|-------+---------|
| 20640 |      10 |


I'll print the median for each column except the last (since it's non-numeric).

#+BEGIN_SRC ipython :session housing :results output raw :exports results
first = housing.columns[:4]
last = housing.columns[4:-1]

def print_median(columns):
    print("|" + "|".join(columns) + "|")
    print("|" + "-+-" * len(columns) + "|")
    print("|" + "|".join([" {:.2f}".format(housing[column].median()) for column in columns]) + "|")
    return

print_median(first)
#+END_SRC

#+RESULTS:
| longitude | latitude | housing_median_age | total_rooms |
|-----------+----------+--------------------+-------------|
|   -118.49 |    34.26 |              29.00 |     2127.00 |

#+BEGIN_SRC ipython :session housing :results output raw :exports results
print_median(last)
#+END_SRC

#+RESULTS:
| total_bedrooms | population | households | median_income | median_house_value |
|----------------+------------+------------+---------------+--------------------|
|         435.00 |    1166.00 |     409.00 |          3.53 |          179700.00 |

Here's the description for the =ocean_proximity= variable
#+BEGIN_SRC ipython :session housing :results output raw :exports results
ocean_proximity_description = housing.ocean_proximity.describe()
print(tabulate(dict(Statistic=ocean_proximity_description.index,
                    Value=ocean_proximity_description.values),
               tablefmt="orgtbl",
               headers='keys'))
#+END_SRC

#+RESULTS:
| Statistic |     Value |
|-----------+-----------|
| count     |     20640 |
| unique    |         5 |
| top       | <1H OCEAN |
| freq      |      9136 |

It looks like the most common house location is less than an hour from the ocean.

#+BEGIN_SRC ipython :session housing :results output raw :exports both
print(
    "{:.2f}".format(
        ocean_proximity_description.loc["freq"]/ocean_proximity_description.loc["count"]))
#+END_SRC

#+RESULTS:
0.44

Which makes up about forty-four percent of all the houses.

* References
  - Géron, Aurélien. Hands-on Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems. First edition. Beijing Boston Farnham: O’Reilly, 2017.
