<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Machine Learning Studies (Posts by necromuralist)</title><link>http://necromuralist.github.io/machine-learning-studies/</link><description></description><atom:link href="http://necromuralist.github.io/machine-learning-studies/authors/necromuralist.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2018 &lt;a href="mailto:necromuralist@gmail.com"&gt;necromuralist&lt;/a&gt; &lt;a rel="license" href="http://creativecommons.org/licenses/by/4.0/"&gt;&lt;img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by/4.0/88x31.png" /&gt;&lt;/a&gt;&lt;br /&gt;This work is licensed under a &lt;a rel="license" href="http://creativecommons.org/licenses/by/4.0/"&gt;Creative Commons Attribution 4.0 International License&lt;/a&gt;.</copyright><lastBuildDate>Fri, 03 Aug 2018 02:13:56 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Data Sources</title><link>http://necromuralist.github.io/machine-learning-studies/posts/data-sources/</link><dc:creator>necromuralist</dc:creator><description>&lt;div id="outline-container-org39e72ee" class="outline-2"&gt;
&lt;h2 id="org39e72ee"&gt;Open Repositories&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org39e72ee"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="http://scikit-learn.org/stable/datasets/index.html#datasets"&gt;Sklearn&lt;/a&gt; : not a repository, per se, but these are the datasets that you can get from sklearn.datasets (not all of them are mentioned, though (e.g. &lt;code&gt;fetch_califoria_housing&lt;/code&gt;))&lt;/li&gt;
&lt;li&gt;&lt;a href="https://archive.ics.uci.edu/ml/index.php"&gt;UC Irvine Machine Learning Repository&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.kaggle.com/datasets"&gt;Kaggle Datasets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://registry.opendata.aws/"&gt;Registry of Open Data on AWS&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org0934d0f" class="outline-2"&gt;
&lt;h2 id="org0934d0f"&gt;Data Portals&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org0934d0f"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="http://dataportals.org/"&gt;Data Portals&lt;/a&gt; : A Comprehensive List of Open Data Portals Around the World&lt;/li&gt;
&lt;li&gt;&lt;a href="https://opendatamonitor.eu/frontend/web/index.php?r=dashboard/index"&gt;Open Data Monitor&lt;/a&gt; : Europead Open Data&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.quandl.com/"&gt;Quandl&lt;/a&gt;: Data for investors&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org439ff47" class="outline-2"&gt;
&lt;h2 id="org439ff47"&gt;Pages Listing Data Sets&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org439ff47"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research"&gt;Wikipedia&lt;/a&gt;: List of datasets for machine learning research&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.quora.com/Where-can-I-find-large-datasets-open-to-the-public"&gt;Quora&lt;/a&gt;: Where can I find large datasets open to the public?&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.reddit.com/r/datasets/"&gt;Reddit&lt;/a&gt;: r/datasets&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>data</category><guid>http://necromuralist.github.io/machine-learning-studies/posts/data-sources/</guid><pubDate>Wed, 01 Aug 2018 19:25:56 GMT</pubDate></item><item><title>References</title><link>http://necromuralist.github.io/machine-learning-studies/posts/references/</link><dc:creator>necromuralist</dc:creator><description>&lt;div id="outline-container-orga9826fe" class="outline-2"&gt;
&lt;h2 id="orga9826fe"&gt;Short Codes&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orga9826fe"&gt;
&lt;p&gt;
These are short-hand codes to make it easier to quickly refer to sources.
&lt;/p&gt;

&lt;ul class="org-ul"&gt;
&lt;li&gt;[HOML]: Hands-On Machine Learning with Scikit-Learn and TensorFlow&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgfbc1035" class="outline-2"&gt;
&lt;h2 id="orgfbc1035"&gt;Bibliography&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgfbc1035"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Géron, Aurélien. Hands-on Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems. First edition. Beijing Boston Farnham: O’Reilly, 2017.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>references bibliograpy citations</category><guid>http://necromuralist.github.io/machine-learning-studies/posts/references/</guid><pubDate>Mon, 30 Jul 2018 23:57:06 GMT</pubDate></item><item><title>California Housing Prices</title><link>http://necromuralist.github.io/machine-learning-studies/posts/california-housing-prices/</link><dc:creator>necromuralist</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://necromuralist.github.io/machine-learning-studies/posts/california-housing-prices/#orgea62352"&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://necromuralist.github.io/machine-learning-studies/posts/california-housing-prices/#org1ed1d50"&gt;Imports&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://necromuralist.github.io/machine-learning-studies/posts/california-housing-prices/#orgaaeef10"&gt;Constants&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://necromuralist.github.io/machine-learning-studies/posts/california-housing-prices/#org93ea195"&gt;The Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://necromuralist.github.io/machine-learning-studies/posts/california-housing-prices/#orgd19ec42"&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgea62352" class="outline-2"&gt;
&lt;h2 id="orgea62352"&gt;Introduction&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgea62352"&gt;
&lt;p&gt;
This is an introductory regression problem that uses California housing data from the 1990 census. There's a description of the original data &lt;a href="https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.htm"&gt;here&lt;/a&gt;, but we're using a slightly altered dataset that's &lt;a href="https://github.com/ageron/handson-ml/tree/master/datasets/housing"&gt;on github&lt;/a&gt; (and appears to be mirrored &lt;a href="https://www.kaggle.com/camnugent/california-housing-prices"&gt;on kaggle&lt;/a&gt;). The problem here is to create a model that will predict the median housing value for a census block group (called "district" in the dataset) given the other attributes. The original data is also available from &lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html"&gt;sklearn&lt;/a&gt; so I'm going to take advantage of that to get the description and do a double-check of the model.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org1ed1d50" class="outline-2"&gt;
&lt;h2 id="org1ed1d50"&gt;Imports&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org1ed1d50"&gt;
&lt;p&gt;
These are the dependencies for this problem.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# python standard library
import os
import tarfile
import warnings
warnings.filterwarnings("ignore", message="numpy.dtype size changed")
from http import HTTPStatus

# from pypi
import matplotlib
import pandas
import requests
import seaborn
from sklearn.datasets import fetch_california_housing
from tabulate import tabulate
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgaaeef10" class="outline-2"&gt;
&lt;h2 id="orgaaeef10"&gt;Constants&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgaaeef10"&gt;
&lt;p&gt;
These are convenience holders for strings and other constants so they don't get scattered all over the place.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;class Data:
    source_slug = "../data/california-housing-prices/"
    target_slug = "../data_temp/california-housing-prices/"
    url = "https://github.com/ageron/handson-ml/raw/master/datasets/housing/housing.tgz"
    source = source_slug + "housing.tgz"
    target = target_slug + "housing.csv"
    chunk_size = 128
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org93ea195" class="outline-2"&gt;
&lt;h2 id="org93ea195"&gt;The Data&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org93ea195"&gt;
&lt;p&gt;
We'll grab the data from github, extract it (it's a &lt;code&gt;tgz&lt;/code&gt; compressed tarfile), then make a pandas data frame from it. I'll also download the &lt;code&gt;sklearn&lt;/code&gt; version.
&lt;/p&gt;
&lt;/div&gt;
&lt;div id="outline-container-org0b2d704" class="outline-3"&gt;
&lt;h3 id="org0b2d704"&gt;Downloading the sklearn dataset&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org0b2d704"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sklearn_housing_bunch = fetch_california_housing("~/data/sklearn_datasets/")
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to /home/brunhilde/data/sklearn_datasets/
&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(sklearn_housing_bunch.DESCR)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
California housing dataset.

The original database is available from StatLib

    http://lib.stat.cmu.edu/datasets/

The data contains 20,640 observations on 9 variables.

This dataset contains the average house value as target variable
and the following input variables (features): average income,
housing average age, average rooms, average bedrooms, population,
average occupation, latitude, and longitude in that order.

References
----------

Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,
Statistics and Probability Letters, 33 (1997) 291-297.


&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(sklearn_housing_bunch.feature_names)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']
&lt;/p&gt;

&lt;p&gt;
Now I'll convert it to a Pandas DataFrame.
&lt;/p&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sklearn_housing = pandas.DataFrame(sklearn_housing_bunch.data,
				   columns=sklearn_housing_bunch.feature_names)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
             MedInc      HouseAge      AveRooms     AveBedrms    Population  \
count  20640.000000  20640.000000  20640.000000  20640.000000  20640.000000   
mean       3.870671     28.639486      5.429000      1.096675   1425.476744   
std        1.899822     12.585558      2.474173      0.473911   1132.462122   
min        0.499900      1.000000      0.846154      0.333333      3.000000   
25%        2.563400     18.000000      4.440716      1.006079    787.000000   
50%        3.534800     29.000000      5.229129      1.048780   1166.000000   
75%        4.743250     37.000000      6.052381      1.099526   1725.000000   
max       15.000100     52.000000    141.909091     34.066667  35682.000000   

           AveOccup      Latitude     Longitude  
count  20640.000000  20640.000000  20640.000000  
mean       3.070655     35.631861   -119.569704  
std       10.386050      2.135952      2.003532  
min        0.692308     32.540000   -124.350000  
25%        2.429741     33.930000   -121.800000  
50%        2.818116     34.260000   -118.490000  
75%        3.282261     37.710000   -118.010000  
max     1243.333333     41.950000   -114.310000  
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org44d2659" class="outline-3"&gt;
&lt;h3 id="org44d2659"&gt;Downloading and uncompressing the data&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org44d2659"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def get_data():
    """Gets the data from github and uncompresses it"""
    if os.path.exists(Data.target):
	return

    os.makedirs(Data.target_slug, exist_ok=True)
    os.makedirs(Data.source_slug, exist_ok=True)
    response = requests.get(Data.url, stream=True)
    assert response.status_code == HTTPStatus.OK
    with open(Data.source, "wb") as writer:
	for chunk in response.iter_content(chunk_size=Data.chunk_size):
	    writer.write(chunk)
    assert os.path.exists(Data.source)
    compressed = tarfile.open(Data.source)
    compressed.extractall(Data.target_slug)
    compressed.close()
    assert os.path.exists(Data.target)
    return
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;
Contents of ../data_temp/california-housing-prices/:
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;housing.csv&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orge6cc6a9" class="outline-3"&gt;
&lt;h3 id="orge6cc6a9"&gt;Building the dataframe&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orge6cc6a9"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;housing = pandas.read_csv(Data.target)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
&amp;lt;class 'pandas.core.frame.DataFrame'&amp;gt;
RangeIndex: 20640 entries, 0 to 20639
Data columns (total 10 columns):
longitude             20640 non-null float64
latitude              20640 non-null float64
housing_median_age    20640 non-null float64
total_rooms           20640 non-null float64
total_bedrooms        20433 non-null float64
population            20640 non-null float64
households            20640 non-null float64
median_income         20640 non-null float64
median_house_value    20640 non-null float64
ocean_proximity       20640 non-null object
dtypes: float64(9), object(1)
memory usage: 1.6+ MB
None
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org5049e7c" class="outline-3"&gt;
&lt;h3 id="org5049e7c"&gt;Comparison to Sklearn&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org5049e7c"&gt;
&lt;p&gt;
The dataset seems to differ somewhat from the sklearn description. Instead of &lt;code&gt;total_rooms&lt;/code&gt; they have &lt;code&gt;AveRooms&lt;/code&gt;, for instance. Is this just a problem of names?
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(sklearn_housing.AveRooms.head())
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
0    6.984127
1    6.238137
2    8.288136
3    5.817352
4    6.281853
Name: AveRooms, dtype: float64
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(housing.total_rooms.head())
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
0     880.0
1    7099.0
2    1467.0
3    1274.0
4    1627.0
Name: total_rooms, dtype: float64
&lt;/p&gt;

&lt;p&gt;
So they are different. Let's see if you can get the sklearn values from the original data set.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print((housing.total_rooms/housing.households).head())
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
0    6.984127
1    6.238137
2    8.288136
3    5.817352
4    6.281853
dtype: float64
&lt;/p&gt;

&lt;p&gt;
It looks like the sklearn values are (in some cases) calculated values derived from the original. It makes sense that they changed some of the things (total number of rooms only makes sense if there is the same number of households in each district, for instance), but it would have been better if they documented the changes they made and why they changed it.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orga5496df" class="outline-3"&gt;
&lt;h3 id="orga5496df"&gt;Inspecting the Data&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orga5496df"&gt;
&lt;p&gt;
If you look at the &lt;code&gt;total_bedrooms&lt;/code&gt; count you'll see that it only has 20,433 non-null values, while the rest of the columns have 20,640 values. These were removed to allow experimenting with missing data. The original dataset that was collected for the census had all the values.
&lt;/p&gt;

&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="org-left"&gt;

&lt;col class="org-left"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="org-left"&gt;Column&lt;/th&gt;
&lt;th scope="col" class="org-left"&gt;Has Missing Values&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-left"&gt;longitude&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;latitude&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;housing_median_age&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;total_rooms&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;total_bedrooms&lt;/td&gt;
&lt;td class="org-left"&gt;True&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;population&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;households&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;median_income&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;median_house_value&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;ocean_proximity&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;
It looks like &lt;code&gt;total_bedrooms&lt;/code&gt; is the only column where there's missing data.
&lt;/p&gt;

&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="org-right"&gt;Rows&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;Columns&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-right"&gt;20640&lt;/td&gt;
&lt;td class="org-right"&gt;10&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;


&lt;p&gt;
I'll print the median for each column except the last (since it's non-numeric).
&lt;/p&gt;

&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="org-right"&gt;longitude&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;latitude&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;housing_median_age&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;total_rooms&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-right"&gt;-118.49&lt;/td&gt;
&lt;td class="org-right"&gt;34.26&lt;/td&gt;
&lt;td class="org-right"&gt;29.00&lt;/td&gt;
&lt;td class="org-right"&gt;2127.00&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="org-right"&gt;total_bedrooms&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;population&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;households&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;median_income&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;median_house_value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-right"&gt;435.00&lt;/td&gt;
&lt;td class="org-right"&gt;1166.00&lt;/td&gt;
&lt;td class="org-right"&gt;409.00&lt;/td&gt;
&lt;td class="org-right"&gt;3.53&lt;/td&gt;
&lt;td class="org-right"&gt;179700.00&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;
Here's the description for the &lt;code&gt;ocean_proximity&lt;/code&gt; variable
Looking at the &lt;code&gt;median_income&lt;/code&gt; you can see that it isn't income in dollars.
&lt;/p&gt;

&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="org-left"&gt;

&lt;col class="org-right"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="org-left"&gt;Statistic&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;Value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-left"&gt;count&lt;/td&gt;
&lt;td class="org-right"&gt;20640&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;unique&lt;/td&gt;
&lt;td class="org-right"&gt;5&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;top&lt;/td&gt;
&lt;td class="org-right"&gt;&amp;lt;1H OCEAN&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;freq&lt;/td&gt;
&lt;td class="org-right"&gt;9136&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;
It looks like the most common house location is less than an hour from the ocean.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(
    "{:.2f}".format(
	ocean_proximity_description.loc["freq"]/ocean_proximity_description.loc["count"]))
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
0.44
&lt;/p&gt;

&lt;p&gt;
Which makes up about forty-four percent of all the houses. Here are all the &lt;code&gt;ocean_proximity&lt;/code&gt; values.
&lt;/p&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="http://necromuralist.github.io/machine-learning-studies/posts/california-housing-prices/ocean_proximity.png" alt="ocean_proximity.png"&gt;
&lt;/p&gt;
&lt;/div&gt;


&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="org-left"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="org-left"&gt;Proximity&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;Count&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;Percentage&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-left"&gt;&amp;lt;1H OCEAN&lt;/td&gt;
&lt;td class="org-right"&gt;9136&lt;/td&gt;
&lt;td class="org-right"&gt;44.2636&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;INLAND&lt;/td&gt;
&lt;td class="org-right"&gt;6551&lt;/td&gt;
&lt;td class="org-right"&gt;31.7393&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;NEAR OCEAN&lt;/td&gt;
&lt;td class="org-right"&gt;2658&lt;/td&gt;
&lt;td class="org-right"&gt;12.8779&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;NEAR BAY&lt;/td&gt;
&lt;td class="org-right"&gt;2290&lt;/td&gt;
&lt;td class="org-right"&gt;11.095&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;ISLAND&lt;/td&gt;
&lt;td class="org-right"&gt;5&lt;/td&gt;
&lt;td class="org-right"&gt;0.0242248&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="http://necromuralist.github.io/machine-learning-studies/posts/california-housing-prices/housing_histogram.png" alt="housing_histogram.png"&gt;
&lt;/p&gt;
&lt;/div&gt;


&lt;p&gt;
If you look at the median income plot you can see that it goes from 0 to 15. It turns out that the incomes were re-scaled and limited to the 0.5 to 15 range. The median age and value were also capped, possibly affecting our price predictions.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgd19ec42" class="outline-2"&gt;
&lt;h2 id="orgd19ec42"&gt;References&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgd19ec42"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Géron, Aurélien. Hands-on Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems. First edition. Beijing Boston Farnham: O’Reilly, 2017.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>regression basics</category><guid>http://necromuralist.github.io/machine-learning-studies/posts/california-housing-prices/</guid><pubDate>Mon, 30 Jul 2018 23:54:39 GMT</pubDate></item><item><title>Linear Classification</title><link>http://necromuralist.github.io/machine-learning-studies/posts/Linear-Classification/</link><dc:creator>necromuralist</dc:creator><description>&lt;div&gt;&lt;div class="section" id="imports"&gt;
&lt;h2&gt;1 Imports&lt;/h2&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_970274767e9f46bd9121cdd146e6ae9d-1"&gt;&lt;/a&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt;
&lt;a name="rest_code_970274767e9f46bd9121cdd146e6ae9d-2"&gt;&lt;/a&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.model_selection&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;
&lt;a name="rest_code_970274767e9f46bd9121cdd146e6ae9d-3"&gt;&lt;/a&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.datasets&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;load_breast_cancer&lt;/span&gt;
&lt;a name="rest_code_970274767e9f46bd9121cdd146e6ae9d-4"&gt;&lt;/a&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.linear_model&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;LogisticRegression&lt;/span&gt;
&lt;a name="rest_code_970274767e9f46bd9121cdd146e6ae9d-5"&gt;&lt;/a&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.svm&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;LinearSVC&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="the-data"&gt;
&lt;h2&gt;2 The Data&lt;/h2&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_1a18f5394cd6423e8e2f779facf46d81-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;cancer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;load_breast_cancer&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;a name="rest_code_1a18f5394cd6423e8e2f779facf46d81-2"&gt;&lt;/a&gt;&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cancer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;/pre&gt;&lt;pre class="literal-block"&gt;
dict_keys(['target_names', 'feature_names', 'data', 'DESCR', 'target'])
&lt;/pre&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_d82c9571d55d4377a43be8d1d4a56e92-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cancer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_d82c9571d55d4377a43be8d1d4a56e92-2"&gt;&lt;/a&gt;                                                    &lt;span class="n"&gt;cancer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_d82c9571d55d4377a43be8d1d4a56e92-3"&gt;&lt;/a&gt;                                                    &lt;span class="n"&gt;stratify&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;cancer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="logistic-regression"&gt;
&lt;h2&gt;3 Logistic Regression&lt;/h2&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_03539af2bc934e55a2a693303e669bba-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;logistic_model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;LogisticRegression&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;penalty&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"l1"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_03539af2bc934e55a2a693303e669bba-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;logistic_model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_03539af2bc934e55a2a693303e669bba-3"&gt;&lt;/a&gt;&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Logistic Training Accuracy: {:.2f}"&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;logistic_model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;a name="rest_code_03539af2bc934e55a2a693303e669bba-4"&gt;&lt;/a&gt;&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Logistic Testing Accuracy: {:.2f}"&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;logistic_model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;/pre&gt;&lt;pre class="literal-block"&gt;
Logistic Training Accuracy: 0.97
Logistic Testing Accuracy: 0.92
&lt;/pre&gt;
&lt;p&gt;Depending on the random seed it sometimes does better on the testing than it does on the training set.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_f12abe8729d8428288bc0532e88225cf-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;coefficients&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pandas&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Series&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;logistic_model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;coef_&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;cancer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature_names&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_f12abe8729d8428288bc0532e88225cf-2"&gt;&lt;/a&gt;&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;coefficients&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;pre class="literal-block"&gt;
mean radius                2.257338
mean texture               0.058581
mean perimeter            -0.001644
mean area                 -0.009889
mean smoothness            0.000000
mean compactness           0.000000
mean concavity             0.000000
mean concave points        0.000000
mean symmetry              0.000000
mean fractal dimension     0.000000
radius error               0.000000
texture error              2.657975
perimeter error            0.000000
area error                -0.118846
smoothness error           0.000000
compactness error          0.000000
concavity error            0.000000
concave points error       0.000000
symmetry error             0.000000
fractal dimension error    0.000000
worst radius               1.635063
worst texture             -0.412327
worst perimeter           -0.201013
worst area                -0.022727
worst smoothness           0.000000
worst compactness          0.000000
worst concavity           -4.246229
worst concave points       0.000000
worst symmetry             0.000000
worst fractal dimension    0.000000
dtype: float64
&lt;/pre&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_fc502ec0475d459d973c4e0cb65dc0b2-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cancer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature_names&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_fc502ec0475d459d973c4e0cb65dc0b2-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;non_zero&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;coefficients&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;coefficients&lt;/span&gt;&lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;a name="rest_code_fc502ec0475d459d973c4e0cb65dc0b2-3"&gt;&lt;/a&gt;&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;non_zero&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_fc502ec0475d459d973c4e0cb65dc0b2-4"&gt;&lt;/a&gt;&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;non_zero&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;pre class="literal-block"&gt;
mean radius        2.257338
mean texture       0.058581
mean perimeter    -0.001644
mean area         -0.009889
texture error      2.657975
area error        -0.118846
worst radius       1.635063
worst texture     -0.412327
worst perimeter   -0.201013
worst area        -0.022727
worst concavity   -4.246229
dtype: float64
0.36666666666666664
&lt;/pre&gt;
&lt;p&gt;The model was able to remove 37% of the features.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_eb5b8c47313d463ea9b801d61f4cbab1-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;LogisticRegression&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_eb5b8c47313d463ea9b801d61f4cbab1-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_eb5b8c47313d463ea9b801d61f4cbab1-3"&gt;&lt;/a&gt;&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Training Accuracy: {0:.2f}"&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;a name="rest_code_eb5b8c47313d463ea9b801d61f4cbab1-4"&gt;&lt;/a&gt;&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Testing Accuracy: {0:.2f}"&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;/pre&gt;&lt;pre class="literal-block"&gt;
Training Accuracy: 0.98
Testing Accuracy: 0.95
&lt;/pre&gt;
&lt;p&gt;Using an &lt;em&gt;L2&lt;/em&gt; penalty of 100 improves the accuracy of the model. Increasing "C" means less regularization, so in this case the improvement came from using a more complex model.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="support-vector-machine-classification"&gt;
&lt;h2&gt;4 Support Vector Machine Classification&lt;/h2&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_61407520433d44e5a4065e4cc003d19d-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;power&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_61407520433d44e5a4065e4cc003d19d-2"&gt;&lt;/a&gt;    &lt;span class="n"&gt;penalty&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;power&lt;/span&gt;
&lt;a name="rest_code_61407520433d44e5a4065e4cc003d19d-3"&gt;&lt;/a&gt;    &lt;span class="n"&gt;svc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;LinearSVC&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;penalty&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_61407520433d44e5a4065e4cc003d19d-4"&gt;&lt;/a&gt;    &lt;span class="n"&gt;svc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_61407520433d44e5a4065e4cc003d19d-5"&gt;&lt;/a&gt;    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"C={}"&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;penalty&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;a name="rest_code_61407520433d44e5a4065e4cc003d19d-6"&gt;&lt;/a&gt;    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Training Accuracy: {0:.2f}"&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;svc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;a name="rest_code_61407520433d44e5a4065e4cc003d19d-7"&gt;&lt;/a&gt;    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Testing Accuracy: {0:.2f}"&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;svc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;a name="rest_code_61407520433d44e5a4065e4cc003d19d-8"&gt;&lt;/a&gt;    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;pre class="literal-block"&gt;
C=0.0001
Training Accuracy: 0.93
Testing Accuracy: 0.93

C=0.001
Training Accuracy: 0.93
Testing Accuracy: 0.92

C=0.01
Training Accuracy: 0.70
Testing Accuracy: 0.71

C=0.1
Training Accuracy: 0.94
Testing Accuracy: 0.93

C=1
Training Accuracy: 0.92
Testing Accuracy: 0.92

C=10
Training Accuracy: 0.93
Testing Accuracy: 0.94

C=100
Training Accuracy: 0.86
Testing Accuracy: 0.84

C=1000
Training Accuracy: 0.92
Testing Accuracy: 0.92
&lt;/pre&gt;
&lt;p&gt;Every time I run this it comes out slightly differently, but it seems like most values do pretty well, there's usually only one or two values of &lt;em&gt;C&lt;/em&gt; below 0.92 for the test set.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="tuning-the-penalty"&gt;
&lt;h2&gt;5 Tuning the Penalty&lt;/h2&gt;
&lt;p&gt;The L1 penalty makes use of more of the features so it will generally do better if they are all relevant. The L2 penalty is better for interpreting the important features and will do better if some of the features are in fact not relevant. Unlike &lt;em&gt;alpha&lt;/em&gt; for regression, &lt;em&gt;C&lt;/em&gt; decreases the regularization as it gets bigger. When searching for the best value it can be useful to search a logarithmic space (e.g. 0.001, 0.01, 0.1, 1, 10, 100)&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>classification</category><guid>http://necromuralist.github.io/machine-learning-studies/posts/Linear-Classification/</guid><pubDate>Thu, 13 Jul 2017 19:38:00 GMT</pubDate></item></channel></rss>